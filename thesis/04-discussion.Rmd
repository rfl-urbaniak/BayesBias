---
output:
  pdf_document: default
  html_document: default
---
# Discussion

1. Our proposition - Bayesian method (red) comment on remarks from chapter2 and then; what we do better
- general insights - estimeated coefficients mean - general HPDI and general different similar - after debiasing the same almost for reddit
- details from the words view - questions: analyzing table features; when human slightly closer - unexpected;
describe the features; 3 small csvs; reddit, google reddit-debiased;general human moved closer- not in table but in sentences; 
comments and values that refer to them: (blue and green)
a) google scale
b) associated females
debiased: question about the change
- better but not for muslims
- corpus frequency maybe?
- better for male
- when in gender different and associated together maybe it is better - gender is very specific, words are about professions; 
- when in religion it is bad different and associated
-  in race it is not obvious - mixed sentiments 

black remarks then:
- mess, change protected words; relative differences stay the same, by the way lipstick on a pig
- word lists are strange, not enough words, large HPDI

4. Summary (how abstract ends)



It is worth diving deeper into some observations that arise while analyzing the results. 
<!-- Looking into specific values -->
As we pointed out, if there is no control of what cosine distance values individual word pairs have, then there is less
understanding of the exact relations between words. If the protected words that we chose do not have high similarity with harmful stereotypes, then one should consider at least two scenarios. The first one is that the choice of protected words and attributes may be corrupted. The second one is that the metric is not able to catch the hidden bias properly. In both cases it is essential to take a look at the individual values before averaging them or aggregating in other ways. 


<!-- Problem with gender words --> 
One has to also remember about the specific situation present in gender dataset. Surprisingly one could observe there high cosine distances values between some female stereotypical professions and male protected words. This leads to new ideas of how to understand the bias origins and its proper detection. To start with, cosine distance seems to catch the information regarding words co-occurrence and not the semantic similarity. If a word stereotypically associated with females has high cosine distance value for male protected words, then the metric is not proper to establish the bias presence as it can be misleading. What we mean is that in same cases it can unnecessarily label words as biased while omitting other more harmful associations. What is more, this highlights again the need for control group. One could check each stereotypical attribute with each protected word but not in order to average the values. One should rather plot the distributions and investigate how the metric measures vectors geometry and hidden semantic information.


<!-- debiasing and uncertainty -->
Debiasing with the method provided by @Manzini2019blackToCriminal shows how it is unpredictable in terms of the results. 
In some cases the final cosine distance is indeed close to neutral words, but in other cases the change is minor. What is more, the inclusion of the uncertainty shows how in some cases it is not clear why a word is according to this method
classified as anomalous. The fact that uncertainty for `assosiated` and `different` class is so high leads to the situation where in the boundaries of an uncertainty of some attributes, one can find the neutral words as well. It makes the usage and interpretation of this dataset and metric extremely complicated and unclear. 

<!-- debiasing --> 
Additionally, one cannot be sure if the bias is still preserved after the debiasing. The fact that all of the cosine distances for protected words and harmful attributes moved to the right, does not mean that the bias is removed. One could argue that maybe it is not clear how to measure bias presence and removal but the method for debiasing works fine. Notwithstanding, it is showed in researches such as @Gonen2019Lipstick, that the bias can hide in the vector geometry and preserve even after applying popular debiasing methods. Therefore it seems to not be justified to claim that the method works properly without precise methods to verify this. 

<!-- no baseline for the interpretation of the results -->
One should remember that there is no baseline for the interpretation of the averaged cosine distances in @Manzini2019blackToCriminalToCriminal. One may assume that if the cosine distance is close to 1 then it is a desired outcome as it means, according to cosine distance assumptions, that there is almost no similarity between the words. However what does it mean to be close to 0? If the averaged cosine distance is equal to 80, then should we still debiase it? It is unclear what the criteria are. On one hand, it seems to be beneficial when the outcome is simplified as it is easier to compare results with one value per set. On the other hand, it is prone to misunderstanding of how to interpret the results and what threshold to assume.


<!-- Baysian method --> 
We propose the use of Bayesian method to introduce the uncertainty measure in bias detection. There are a few advantages when including this method in the bias analysis. As noted before in our example with religion dataset, one may obtain new insight into the bias issue after analyzing the mean estimates of the coeï¬€icients. There seems to be a minor difference for the mean concerning the `assosiated` class. This can lead to the conclusions that we do not have strong reasons to perceive lower cosine distance as significantly related to the fact that the attribute is associated. 

One may dive deeper into this idea by investigating the connection coefficients for individual words with the use of the visualization. There one observes that indeed most of the words are clustered together independently of the class that they belong to. It is of course partly due to the fact that we include uncertainty now. This example shows how understanding of the bias measurement may differ when switching to Bayesian method. One could argue that after this analysis one obtains more information on the dataset and can pursue with the further research more cautiously.


<!-- 
remarks:
- delete papers from github/hide them
- should we quote "debiasing" or without quoting

-->

<!-- textbf{Control group} 

How to properly prepare control group in terms of quality and quantity?
<!-- + check if it may be proved in the code --> 

<!--  \textbf{Verifying other similarity meaures} 

Besides cosine similarity, there are other methods used to find the similarity between vectors.  

a) TS-SS 
todo: check if it makes sense to calculate for all of the datasets; interpretations etc.
https://www.researchgate.net/publication/303513110_A_Hybrid_Geometric_Approach_for_Measuring_Similarity_Level_Among_Documents_and_Document_Clustering

b) WEAT test 
<!-- https://docs.responsibly.ai/word-embedding-bias.html#module-responsibly.we.weat -->

<!-- [to check https://github.com/taki0112/Vector_Similarity] -->

<!-- TODO: check in code if they make more sense -->



<!-- ## Bibliography
- https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&qid=1502062931&sr=8-1&keywords=Neural+Network+Methods+in+Natural+Language+Processing&linkCode=sl1&tag=inspiredalgor-20&linkId=d63df073fea3ebe2d405820570b3ff03 ->


<!-- ## *** Additional -->

<!-- ### Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings" -->
<!-- [article https://arxiv.org/pdf/1607.06520.pdf] -->

<!-- Assumptions: -->
<!-- "We assume there is a set of gender neutral words $N\subseteq W$ such as flight attendant or shoes, which, by definition, are not specific to any gender." -->

<!-- TODO: check if it is useful for the analysis of black is .. -->


<!-- ### Identifying the bias subspace  -->

<!-- Interesting analysis about limits o subspace and research [source https://kawine.github.io/blog/nlp/2019/09/23/bias.html] -->

<!-- Resources to analyze: -->
<!-- 1. https://arxiv.org/pdf/2005.00965.pdf -->
<!-- 2. https://arxiv.org/pdf/2009.09435.pdf -->
<!-- 3. https://arxiv.org/pdf/1904.04047.pdf -->
<!-- 4. https://www.aclweb.org/anthology/P19-1166.pdf -->


<!-- ### How is cosine similarity entangled with analogy topic in NLP (is it??) -->

<!-- article:  https://arxiv.org/pdf/1905.09866.pdf -->

<!-- Critique of the usage of analogies as a proof for gender biases -->

<!-- TODO: check if there is other argument than that algorithm does not allow it -->

<!-- general thoughts 
https://www.researchgate.net/publication/349408535_Robustness_and_Reliability_of_Gender_Bias_Assessment_in_Word_Embeddings_The_Role_of_Base_Pairs

-
-
-
-->