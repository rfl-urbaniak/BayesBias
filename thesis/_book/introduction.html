<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Conceptual and methodological problems with bias detection and avoidance in natural language processing</title>
  <meta name="description" content="Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  
  
  

<meta name="author" content="Alicja Dobrzeniecka" />


<meta name="date" content="2021-06-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="cosine-similarity-and-bias-detection.html"/>
<script src="book_assets/header-attrs-2.8/header-attrs.js"></script>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="book_assets/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="book_assets/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html"><i class="fa fa-check"></i><b>2</b> Cosine similarity and bias detection</a>
<ul>
<li class="chapter" data-level="2.1" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#word-embeddings"><i class="fa fa-check"></i><b>2.1</b> Word embeddings</a></li>
<li class="chapter" data-level="2.2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-similarity-and-distance"><i class="fa fa-check"></i><b>2.2</b> Cosine similarity and distance</a></li>
<li class="chapter" data-level="2.3" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-distance-in-a-one-class-bias-detection"><i class="fa fa-check"></i><b>2.3</b> Cosine distance in a one-class bias detection</a></li>
<li class="chapter" data-level="2.4" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-distance-in-a-multi-class-bias-detection"><i class="fa fa-check"></i><b>2.4</b> Cosine distance in a multi-class bias detection</a></li>
<li class="chapter" data-level="2.5" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#limitations-of-the-approach"><i class="fa fa-check"></i><b>2.5</b> Limitations of the approach</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html"><i class="fa fa-check"></i><b>3</b> Walkthrough with the religion dataset</a>
<ul>
<li class="chapter" data-level="3.1" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#loading-and-understanding-the-dataset"><i class="fa fa-check"></i><b>3.1</b> Loading and understanding the dataset</a></li>
<li class="chapter" data-level="3.2" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#first-look-at-the-empirical-distributions"><i class="fa fa-check"></i><b>3.2</b> First look at the empirical distributions</a></li>
<li class="chapter" data-level="3.3" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#looking-at-the-islam-related-words"><i class="fa fa-check"></i><b>3.3</b> Looking at the islam-related words</a></li>
<li class="chapter" data-level="3.4" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#bayesian-model-structure-and-assumptions"><i class="fa fa-check"></i><b>3.4</b> Bayesian model structure and assumptions</a></li>
<li class="chapter" data-level="3.5" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#choosing-predictors"><i class="fa fa-check"></i><b>3.5</b> Choosing predictors</a></li>
<li class="chapter" data-level="3.6" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#dataset-level-coefficients"><i class="fa fa-check"></i><b>3.6</b> Dataset-level coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html"><i class="fa fa-check"></i><b>4</b> Protected-word level analysis</a>
<ul>
<li class="chapter" data-level="4.1" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html#model-structure-and-assumptions"><i class="fa fa-check"></i><b>4.1</b> Model structure and assumptions</a></li>
<li class="chapter" data-level="4.2" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html#protected-classes-in-reddit-and-google-embeddings"><i class="fa fa-check"></i><b>4.2</b> Protected classes in Reddit and Google embeddings</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html"><i class="fa fa-check"></i><b>5</b> The role of debiasing</a>
<ul>
<li class="chapter" data-level="5.1" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html#dataset-level-coefficients-after-debiasing"><i class="fa fa-check"></i><b>5.1</b> Dataset-level coefficients after debiasing</a></li>
<li class="chapter" data-level="5.2" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html#protected-classes-after-debiasing"><i class="fa fa-check"></i><b>5.2</b> Protected classes after debiasing</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Conceptual and methodological problems with bias detection and avoidance in natural language processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Conceptual and methodological problems with bias detection and avoidance in natural language processing</h1>
<p class="author"><em>Alicja Dobrzeniecka</em></p>
<p class="date" style="margin-top: 1.5em;"><em>2021-06-10</em></p>
</div>
<div id="introduction" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Introduction</h1>
<!--### Articles:

  1. "Are We Consistently Biased? Multidimensional Analysis of Biases in Distributional Word Vectors"
  https://www.aclweb.org/anthology/S19-1010.pdf 

  2. Proving the harmfullness of biases in models 
[source https://poseidon01.ssrn.com/delivery.php?ID=004020091089126117028081015085009075120015077012021005105005104113074127101113104030041107053119007034112081087110119071025006053026022082043000071011123067077124004027040024008122116091025075126111098104007096110113089007070120009111119015084097017092&EXT=pdf&INDEX=TRUE]

  3. Summary what causes bias short article
  [source https://kawine.github.io/blog/nlp/2019/09/23/bias.html]-->
<!-- Men Also Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraint -->
<!-- Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting -->
<!-- !bibliografia paperdraft; jabref
4.pomysly na czwarty rozdzial - podsumowanie; 
jak mierzyc bias za pomoca jednej liczby?
IAT - test psychologiczny -->
<!-- 
    1.state the general topic and give some background
    2.provide a review of the literature related to the topic
    3.define the terms and scope of the topic
    4.outline the current situation
    5.evaluate the current situation (advantages/ disadvantages) and identify the gap
    6.identify the importance of the proposed research
    7.state the research problem/ questions
    8.state the research aims and/or research objectives
    9.state the hypotheses
    10.outline the order of information in the thesis
    11.outline the methodology
-->
<!-- \textbf{state the general topic and give some background}-->
<p>Natural language processing (NLP) is a subfield of computer science that processes and analyzes language in text and speech
with the use of modern programming methods. It has practical applications in everyday life as it concerns tasks such as email filters,
smart assistants, search results, language translations, text analytics and so on. Models used to accomplish these tasks need a lot of
data to learn from. This data originates from humans activities and historical recordings such as texts, messages or speeches. It turns out
that in the learning process these models can learn implicit biases that reflect harmful stereotypical thinking still present in modern societies. One can find methods that aim at identifying and measuring hidden biases and/or try to remove them by modifying the models.
There are many different types of models in NLP depending on a task that they are supposed to solve. However, all of them need as an input words represented by means of numbers and this is accomplished with word embedding models. The models usually assign the values based on the context in which the words appear. It means that the input data can have enormous influence on the outcome. The biases seem to have their primary source in the way the words are assigned the numerical values.</p>
<!-- \textbf{review of the literature related to the topic}-->
<p>There is considerable amount of literature available on the topic of bias detection and mitigation in NLP models. <span class="citation"><a href="#ref-Bolukbasi2016Man" role="doc-biblioref">Bolukbasi, Chang, Zou, Saligrama, &amp; Kalai</a> (<a href="#ref-Bolukbasi2016Man" role="doc-biblioref">2016</a>)</span> focuses on gender biases that may be observable while investigating the representation of job occupations and gender in terms of their assigned numerical
values. The authors apply cosine similarity measurement to investigate the phenomenon where (the vectors corresponding to) words related to jobs that are stereotypically associated with a given gender are in fact in the model situated closer to this gender.
They also use analogy tasks to evaluate if the bias is present in the word embedding model. They check analogies by comparing pairs of word vectors, for example they search for the word complementing the puzzle: man is to doctor as woman is to …? First they subtract word “man” from word “woman” and then
they search for the ranked list of other words pairs that have similar vectors’ difference. They also include in the formula a threshold to ensure that the resulting pairs could not be randomly picked.</p>
<p>However, as in <span class="citation"><a href="#ref-Nissim2019Fair" role="doc-biblioref">Nissim, Noord, &amp; Goot</a> (<a href="#ref-Nissim2019Fair" role="doc-biblioref">2019</a>)</span> it is pointed out, there are some limitations of this approach. According to the authors in practice most of analogies implementations do not return any input words. This means that it does not make sense to expect the algorithm to return the same profession for both woman and man. Therefore this method seems to be limited in terms of bias detection. The other problems regard for example the
choice of pairs and words that are used to detect the presence of discrimination as it is often subjective and without proper justification. Additionally the choice of parameter set in <span class="citation"><a href="#ref-Bolukbasi2016Man" role="doc-biblioref">Bolukbasi, Chang, Zou, Saligrama, &amp; Kalai</a> (<a href="#ref-Bolukbasi2016Man" role="doc-biblioref">2016</a>)</span> formula to ensure that word pairs are not picked by random, is also not justified and changing it drastically
influences the results.</p>
<p><span class="citation"><a href="#ref-Caliskan2017Semantics" role="doc-biblioref">Islam, Bryson, &amp; Narayanan</a> (<a href="#ref-Caliskan2017Semantics" role="doc-biblioref">2016</a>)</span> touches upon the topic of biases regarding race and gender. They apply knowledge from well-known psychological studies such as Implicit Association Test to research the relation between human stereotypical thinking and model learnt biases to
discover close relationship between these two. For the evaluation they use Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT).</p>
<p><span class="citation"><a href="#ref-manzini2019black" role="doc-biblioref">Manzini, Lim, Tsvetkov, &amp; Black</a> (<a href="#ref-manzini2019black" role="doc-biblioref">2019</a>)</span> proposes a novel way of using cosine similarity to obtain the information on assumed resemblance between words. They investigate
an approach that enables them to measure the bias for a class (like gender, religion, race) and express the final result with a single metric.</p>
<!-- Nissim2019Fair -->
<!-- \textbf{define the terms and scope of the topic}-->
<p>It is worth noticing the general distinction of biases mentioned in <span class="citation"><a href="#ref-Caliskan2017Semantics" role="doc-biblioref">Islam, Bryson, &amp; Narayanan</a> (<a href="#ref-Caliskan2017Semantics" role="doc-biblioref">2016</a>)</span>. They refer to the publication concerning Implicit Association Test (Greenwald et al., 1998) that measures the strength of associations between concepts or stereotypes by calculating the time of reaction for special tasks. It is worth noticing that humans naturally exhibit some biases and that they do not always cause social concern. One can imagine the intuitive associations between for example insects and flowers, and the feelings of pleasantness or unpleasantness. In general, people would rather associate flowers with feeling pleasant than insects, and this preference could be named a bias or
prejudice in some direction. However, this type of preference does not cause an uproar and is a rather morally neutral case. Unfortunately, there are other
biases and prejudices that directly influence the quality of other people’s lives and therefore they should be taken care of.</p>
<p>One can find various definitions trying to capture what bias and fairness actually are. With the choice of the definition, implications into the real-life applications may change as well. <span class="citation"><a href="#ref-Mehrabi2019Survey" role="doc-biblioref">Mehrabi, Morstatter, Saxena, Lerman, &amp; Galstyan</a> (<a href="#ref-Mehrabi2019Survey" role="doc-biblioref">2019</a>)</span> mark out that there exist different types of biases such as historical bias, representation bias, measurement bias (the list is long). This indicates how complex the issue of bias is. Without the proper understanding and awareness of the problem, people are prone to unconsciously sustain the bias existence.</p>
<p><span class="citation"><a href="#ref-Mehrabi2019Survey" role="doc-biblioref">Mehrabi, Morstatter, Saxena, Lerman, &amp; Galstyan</a> (<a href="#ref-Mehrabi2019Survey" role="doc-biblioref">2019</a>)</span> also distinguish different types of discrimination, some of them will be briefly described. By protected attributes we mean those qualities, traits or characteristics that one cannot legally discriminate against.
Direct discrimination occurs when protected attributes of individuals explicitly result in non-favorable outcomes toward them. In contrast in indirect discrimination individuals appear to be treated equally but anyway they end up being treated unjustly due to the hidden effects of biases towards their protected attributes. Systemic discrimination takes place when policies, customs or behaviors that result from certain culture or organizational structure lead to discrimination against some groups of people. Finally, very common statistical discrimination refers to using
average group statistics to judge person belonging to the group.</p>
<p>The topic of discrimination is entangled with another concept which is fairness. It is essential to grasp some concepts of fairness to take them into
consideration while designing implementation of some machine learning model. In Mehrabi2019Survey one may notice that depending on the context and application different definitions may be applied.</p>
<!--\textbf{outline the current situation} -->
<p>The most popular methods focus on comparing the similarity between words from protected groups and those that are considered to be stereotypical or harmful in some way. One can find in this group methods such as euclidean distance or cosine similarity (which is equivalent to dot product if the
vectors are normalized). There are also other ways to detect the effects of biases. For example through the investigation of the model performance on certain tasks that validate if the model returns some values
independently on gender or race or not.</p>
<!-- \textbf{evaluate the current situation (advantages/ disadvantages) and identify the gap}-->
<p>The currently used methods (such as cosine similarity) make the similarity values often aggregated in a way that may lead to hasty conclusions. The averaging of values and the lack of uncertainty may lead to the incomplete picture of the bias situation in the vocabulary.</p>
<!-- \textbf{identify the importance of the proposed research}-->
<p>One can find a number of articles on negative real-life implications resulting from the presence of unaddressed biases in the machine learning models.</p>
<!-- \textbf{state the research problem/ questions} -->
<p>In the paper we indicate how current methods used to detect biases in natural language models are limited from the perspective of Bayesian analysis.</p>
<!-- \textbf{state the research aims and/or research objectives} -->
<p>Our research enhances the current way in which the bias detection is performed to make sure that it is
methodologically valid.</p>
<!-- \textbf{state the hypotheses} -->
<p>The key hypothesis is that greater understanding of data and bias implications can be achieved when Bayesian methods are applied to issue.</p>
<!-- \textbf{outline the order of information in the thesis} -->

</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Bolukbasi2016Man" class="csl-entry">
Bolukbasi, T., Chang, K.-W., Zou, J. Y., Saligrama, V., &amp; Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. <em>CoRR</em>, <em>abs/1607.06520</em>. Retrieved from <a href="http://arxiv.org/abs/1607.06520">http://arxiv.org/abs/1607.06520</a>
</div>
<div id="ref-Caliskan2017Semantics" class="csl-entry">
Islam, A. C., Bryson, J. J., &amp; Narayanan, A. (2016). Semantics derived automatically from language corpora necessarily contain human biases. <em>CoRR</em>, <em>abs/1608.07187</em>. Retrieved from <a href="http://arxiv.org/abs/1608.07187">http://arxiv.org/abs/1608.07187</a>
</div>
<div id="ref-manzini2019black" class="csl-entry">
Manzini, T., Lim, Y. C., Tsvetkov, Y., &amp; Black, A. W. (2019). Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings. Retrieved from <a href="http://arxiv.org/abs/1904.04047">http://arxiv.org/abs/1904.04047</a>
</div>
<div id="ref-Mehrabi2019Survey" class="csl-entry">
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &amp; Galstyan, A. (2019). A survey on bias and fairness in machine learning. <em>CoRR</em>, <em>abs/1908.09635</em>. Retrieved from <a href="http://arxiv.org/abs/1908.09635">http://arxiv.org/abs/1908.09635</a>
</div>
<div id="ref-Nissim2019Fair" class="csl-entry">
Nissim, M., Noord, R. van, &amp; Goot, R. van der. (2019). Fair is better than sensational: Man is to doctor as woman is to doctor. <em>CoRR</em>, <em>abs/1905.09866</em>. Retrieved from <a href="http://arxiv.org/abs/1905.09866">http://arxiv.org/abs/1905.09866</a>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="cosine-similarity-and-bias-detection.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
