<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2 Cosine similarity and bias detection | Conceptual and methodological problems with bias detection and avoidance in natural language processing</title>
  <meta name="description" content="2 Cosine similarity and bias detection | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="2 Cosine similarity and bias detection | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2 Cosine similarity and bias detection | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  
  
  

<meta name="author" content="Alicja Dobrzeniecka" />


<meta name="date" content="2021-04-28" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="understanding-the-method-of-gender-bias-detection.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.0.1" data-path="introduction.html"><a href="introduction.html#articles"><i class="fa fa-check"></i><b>1.0.1</b> Articles:</a></li>
<li class="chapter" data-level="1.0.2" data-path="introduction.html"><a href="introduction.html#questions-to-answer-in-the-introduction"><i class="fa fa-check"></i><b>1.0.2</b> Questions to answer in the introduction:</a></li>
<li class="chapter" data-level="1.0.3" data-path="introduction.html"><a href="introduction.html#general-technical-questions"><i class="fa fa-check"></i><b>1.0.3</b> General technical questions:</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html"><i class="fa fa-check"></i><b>2</b> Cosine similarity and bias detection</a><ul>
<li class="chapter" data-level="2.1" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#word-embeddings"><i class="fa fa-check"></i><b>2.1</b> Word embeddings</a></li>
<li class="chapter" data-level="2.2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-similarity-and-distance"><i class="fa fa-check"></i><b>2.2</b> Cosine similarity and distance</a></li>
<li class="chapter" data-level="2.3" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-distance-in-a-multi-class-bias-detection"><i class="fa fa-check"></i><b>2.3</b> Cosine distance in a multi-class bias detection</a></li>
<li class="chapter" data-level="2.4" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#limitations-of-the-approach"><i class="fa fa-check"></i><b>2.4</b> Limitations of the approach</a></li>
<li class="chapter" data-level="2.5" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#one-class-gender-bias-with-cosine-similarity-a-case-study"><i class="fa fa-check"></i><b>2.5</b> One class gender bias with cosine similarity: a case study</a></li>
<li class="chapter" data-level="2.6" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#bibliography"><i class="fa fa-check"></i><b>2.6</b> Bibliography</a></li>
<li class="chapter" data-level="2.7" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#walkthrough-with-the-religion-dataset"><i class="fa fa-check"></i><b>2.7</b> Walkthrough with the religion dataset</a></li>
<li class="chapter" data-level="2.8" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#control-groups-in-the-protectedword-position"><i class="fa fa-check"></i><b>2.8</b> Control groups in the protectedWord position</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="understanding-the-method-of-gender-bias-detection.html"><a href="understanding-the-method-of-gender-bias-detection.html"><i class="fa fa-check"></i><b>3</b> Understanding the method of gender bias detection</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Conceptual and methodological problems with bias detection and avoidance in natural language processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="cosine-similarity-and-bias-detection" class="section level1">
<h1><span class="header-section-number">2</span> Cosine similarity and bias detection</h1>
<div id="word-embeddings" class="section level2">
<h2><span class="header-section-number">2.1</span> Word embeddings</h2>
<!-- [based on https://machinelearningmastery.com/what-are-word-embeddings/] -->
<p>To get a proper understanding of what cosine similarity measurement is, one first needs to be introduced with the concept of word embeddings. Word embeddings are a class of various techniques that allow us to represent words as vectors. This learned representation of text has certain properties - such as storing similar words close to each other in a vector space. Word embeddings return a dense distributed representation of individual words which in reality might involve from tens to hundreds of dimensions. One can understand distributed representation as an opposite of localist representation used in one-hot encoding. In the latter, each vector contains information only about a single data point. It is achieved by first mapping categorical values (words) to integer ones and then for each integer value a binary vector is assigned. The vector has only 0â€™s except for the index of the integer which is given 1.</p>
<p>An example of localist representation:</p>
<p><code>cat -&gt; [0 0 0 0 0 1]</code></p>
<p>In contrast to this process, distributed representation returns vectors that have continuous values instead of discrete 1s and 0s.</p>
<p>An example of distributed representation:</p>
<p><code>cat -&gt; [0.345 0.345 0.567 0.413 0.390 0.124]</code></p>
<p>One of the advantages of using distributed representation is that one is able to represent an enormous amount of concepts with a smaller number of units. It is also able to indicate similarities better as words of similar meanings can have similar numeric vectors.</p>
<p>Word embeddings have many applications in natural language processing area. They are handy in document search and information retrieval. They also have their part in improving automatic translations. Additionally well-learnt word representations may contribute to the betterment of sentiment analysis or spam detection.</p>
</div>
<div id="cosine-similarity-and-distance" class="section level2">
<h2><span class="header-section-number">2.2</span> Cosine similarity and distance</h2>
<!-- [based on https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity] -->
<p>Cosine similarity is a method for finding whether vector representations for two words suggest that they are similar or somehow connected. Cosine similarity is the cosine of the angle between two vectors: the result of dividing dot product of the vectors by the product of their magnitudes.</p>
<span class="math display">\[\begin{align} \tag{Sim}
\mathsf{cosineSimilarity}(A,B) &amp; = \frac{A \cdot B}{\vert \vert A \vert \vert \,\vert \vert B \vert \vert}
\end{align}\]</span>
<p>Cosine is a proper tool for this operation as its result refers directly to the nature of vectors and additionally may be easily interpreted. Using this scale, one can compare vectors similarities in a clear manner. When the vectors are aligned perpendicularly to each other then their similarity equals 0 which is the same as cosine from 90 degrees. It tells us that the similarity is very insignificant. And as the angle between them gets smaller and smaller, the value from cosine application is approaching one, which stands for the greatest similarity.</p>
<p>One of this measure limitation is that it informs us only about similarities between vectors in terms of their orientation. However it seems that in comparing words in terms of this metric, the magnitude of vectors may be treated as irrelevant, as the most important information pertains to the direction of the vectors. This relies on the assumption that similarity is greatest when vectors occupy the same relative proportion of each vector subspace.</p>
<p>In what follows, it is important to distinguish between cosine similarity and cosine distance concepts. Cosine distance formula is simply:</p>
<span class="math display">\[\begin{align} \tag{Sim}
\mathsf{cosineDistance}(A,B) &amp;  = 1 - \mathsf{cosineSimilarity}(A,B)\\
 &amp;  = 1 - \frac{A \cdot B}{\vert \vert A \vert \vert \,\vert \vert B \vert \vert} \nonumber
\end{align}\]</span>
<p>The greater the similarity between two vectors the smaller the distance between them.</p>
</div>
<div id="cosine-distance-in-a-multi-class-bias-detection" class="section level2">
<h2><span class="header-section-number">2.3</span> Cosine distance in a multi-class bias detection</h2>
<!-- [article https://arxiv.org/pdf/1904.04047.pdf] -->
<p>In the article &quot;Black is to Criminal as Caucasian is to Police: Detecting and Removing Multiclass Bias in Word Embeddings&quot; the authors present an interesting approach towards finding similarities between classes of words.</p>
<p>The authors claim that available online texts are full of direct or indirect human stereotypes. As a result, word embeddings are prone to learn and maybe amplify those biases and propagate them further into AI models that are used for various applications. Cosine distance is used in the article as a measure to first prove multi-class bias existence and then to show how through bias mitigation techniques it may be decreased. There are various steps in the process but the main once are shortly described below.</p>
<ol style="list-style-type: lower-alpha">
<li>Mean cosine distance:</li>
</ol>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
<span class="kw">def</span> _unary_s(embeddings, target, attributes):
    <span class="cf">return</span> np.mean([spatial.distance.cosine(embeddings[target],
    embeddings[ai]) <span class="cf">for</span> ai <span class="kw">in</span> attributes])</code></pre></div>
<p>It is used to identify similarities between protected words and a list of attributes. After finding mean distance between each protected word a number of other words, one takes a mean of all of those means to return one value for a whole class (like eg. religion class biases).</p>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Principal component analysis is used to identify bias subspace. In details the bias subspace process is as follows:</p>
<ol style="list-style-type: decimal">
<li>Create list of embeddings for eg. he/she word</li>
<li>Take a mean of these embeddings</li>
<li>Add to a list</li>
<li>Repeat for all groups (like his/her, brother/sister etc.)</li>
<li>From each pair substract their mean</li>
<li>Create matrix of all pairs (if 7 pairs with 50 dimensions each, then matrix is equal to 14x50)</li>
<li>Use PCA on matrix with n_components=2</li>
<li>The returned pca.components_ is a bias subspace</li>
</ol></li>
<li><p>Bias component is removed by using either hard debiasing or soft one. Hard debiasing is a proces of completely remove subspace components from embedding; it consists of two steps, first from neutral words the bias components are removed, second gendered word embeddings have their bias components equalized, which means that for example in terms of &quot;man&quot; and &quot;woman&quot; vectors, their bias components should be in opposite directions with same length of magnitude. In turn, soft debiasing is a partial removal of subspace components from embedding.</p></li>
</ol>
<p>How are the multi-class biases actually defined? The authors take a list of alleged biases towards protected groups like gender, race, religion from various scientific reports. The multi-class bias existence is further proved by providing the reader with the value of a mean from the cosine similarity between protected word and list of of previously defined biases.</p>
<!-- The methodology used in the paper  -->
<p>Now let us analyse the details of the solution that uses cosine similarity. As the code for the whole solution is provided I was able to reconstruct the exact same results that they obtained. Having the source code enabled me to see the details behind solution and analyse each step and each calculated value to state own claims and doubts. The below analysis concerns the usage of cosine similarity in proving biases and their mitigation.</p>
<p>The example refers to the process of hard debiasing on religious attributes.</p>
<ol style="list-style-type: decimal">
<li>First we load word embeddings from reddit.US.txt.tok.clean.cleanedforw2v.w2v dataset</li>
</ol>
<ul>
<li>word embeddings have 50 dimensions only</li>
<li>the amount of individual words from the dataset equals 44895</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>Words that are supposed to be neutral but are biased towards some religion:</li>
</ol>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">[<span class="st">&#39;greedy&#39;</span>, <span class="st">&#39;cheap&#39;</span>, <span class="st">&#39;hairy&#39;</span>, <span class="st">&#39;liberal&#39;</span>, <span class="st">&#39;judgemental&#39;</span>, <span class="st">&#39;conservative&#39;</span>,
 <span class="st">&#39;familial&#39;</span>, <span class="st">&#39;violent&#39;</span>, <span class="st">&#39;terrorist&#39;</span>, <span class="st">&#39;dirty&#39;</span>, <span class="st">&#39;uneducated&#39;</span>]</code></pre></div>

<ol start="3" style="list-style-type: decimal">
<li>On the bases of definite sets the bias subspace is defined:</li>
</ol>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">[[<span class="st">&quot;judaism&quot;</span>, <span class="st">&quot;christianity&quot;</span>, <span class="st">&quot;islam&quot;</span>],
    [<span class="st">&quot;jew&quot;</span>, <span class="st">&quot;christian&quot;</span>, <span class="st">&quot;muslim&quot;</span>],
  [<span class="st">&quot;synagogue&quot;</span>, <span class="st">&quot;church&quot;</span>, <span class="st">&quot;mosque&quot;</span>],
  [<span class="st">&quot;torah&quot;</span>, <span class="st">&quot;bible&quot;</span>, <span class="st">&quot;quran&quot;</span>],
  [<span class="st">&quot;rabbi&quot;</span>, <span class="st">&quot;priest&quot;</span>, <span class="st">&quot;imam&quot;</span>]]</code></pre></div>

<ol start="4" style="list-style-type: decimal">
<li><p>Next function <code>neutralize_and_equalize</code> is applied. From previously defined neutral words only the bias subspace is substracted. The second step concerns equalizing protected words (like christianity, islam etc.) themselves.</p></li>
<li><p>Then there is a multiclass evaluation. They check each protected attribute with each group of &quot;biased neutral words.&quot; Then they do each time mean from the cosine distances from protected word to neutral words. Then they do a mean from all of these means to return one value. If the returned value of cosine distance is greater than the one calculated at the very beginning then it means that the bias was removed as the similarity between stereotypical word and the protected one decreased.</p></li>
</ol>
</div>
<div id="limitations-of-the-approach" class="section level2">
<h2><span class="header-section-number">2.4</span> Limitations of the approach</h2>

<!-- TODO: Universal ways to decide on the quality and type of attributes so that the research is significant? -->
<p>The attributes are taken from different sources, therefore there is no one justification for their choice.</p>

<!-- TODO: Universal ways to decide on the number of attributes so that proving bias is significant? -->
<!-- https://cs.stanford.edu/people/wmorgan/sigtest.pdf -->
<p>In the article there is no mention of methodology for deciding on the number of attributes necessary to prove a hypothesis on the given size of dataset. There are however some ways to estimate how many samples we need to make sure that the result is significant.</p>

<p>The authors use the mean average cosine similarity to check on multi-class similarity between protected word and harmful stereotypes. Details of the process are described as follows.</p>
<p>When one calculates individual cosine similarity between some chosen word like &quot;christianity&quot; and neutral attributes that are supposed to be biased are actually often having very small and negative similarity value. As a result the measurement cosine distance is for such pair of words greater than 1. According to some sources (find sth better than issue on github) when the value exceeds 1 it means that it is not a good measure for the case as the words seem to have no similarity or whatsoever.</p>
<p>Doing a mean hides this issue and as there are pairs having negative and small similarities and there are those that have similarity equal to 0.5, the resulting calculation seems to be in norm.</p>

<p>Assuming for a moment that the value of multi-class cosine distance is correct, one may question the results' interpretation. In [Black is to Criminal as Caucasian is to Police:Detecting and Removing Multiclass Bias in Word Embeddings] Table 2, there are summarized the averages of cosine distance per group (gender, race, religion). I would like to focus now on analyzing the values relating to religious biases. Here is the fragment of table that refers to that:</p>
<table>
<thead>
<tr class="header">
<th>Religion Debiasing</th>
<th>MAC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Biased</td>
<td>0.859</td>
</tr>
<tr class="even">
<td>Hard Debiased</td>
<td>0.934</td>
</tr>
<tr class="odd">
<td>Soft Debiased (<span class="math inline">\(\lambda\)</span> = 0.2)</td>
<td>0.894</td>
</tr>
</tbody>
</table>
<p>MAC stand for mean average cosine similarity although in reality the values of cosine distance are there stored. What may attract attention is the fact that the value of cosine distance in &quot;Biased&quot; category is already quite high even before &quot;debiasing&quot;. High cosine distance indicates low cosine similarity between values. One could think that average cosine similarity equal to approximately 0.141 is not significant enough to consider it as biased. However the authors aim to mitigate &quot;biases&quot; in vectors with such great distance to make it even larger. Methodologically there is a question on what bases this small similarity is still considered as a proof of bias presence.</p>
<!-- TODO: numerical test if this distance is significant? check mean mean cosine distances of some random words? -->

<!-- TODO: -->
<!-- 1. https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec11.pdf -->
<!-- 2. https://stats.stackexchange.com/questions/341535/curse-of-dimensionality-does-cosine-similarity-work-better-and-if-so-why -->
<!-- 3. https://stats.stackexchange.com/questions/21547/distance-metric-and-curse-of-dimensions -->
<!-- + check if it may be proved in the code -->
<p>Curse of dimensionality may take place when there is an increase in volume of data that results in adding extra dimensions to the Euclidean space. According to the article &quot;<a href="https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/" class="uri">https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/</a>&quot; as the number of features increases, it may be harder and harder to get useful information from the data with the usage of available algorithms. One may notice that more data should contribute to greater amount of information but more information also means greater risk of noise and distractions in data. At the same time, many times modern solutions are adapted to smaller dimensions and their results in higher ones are not intuitive or may be prone to be mistaken.</p>
<p>Using cosine similarity in high dimensions in word embeddings may also be prone to the curse of dimensionality. According to this article &quot;<a href="https://www.researchgate.net/publication/327498046_The_Curse_of_Dimensionality_Inside_Out" class="uri">https://www.researchgate.net/publication/327498046_The_Curse_of_Dimensionality_Inside_Out</a>&quot; there are reasons to consider this phenomenon when searching for word similarities in higher dimensions.</p>
<!-- Add more information on the experiment -->
<p>In the article an experiment is conducted that aims at showing how the similarity values and variation change as the number of dimensions increases. The hypothesis made in the paper states that two things will happen as the number of dimensions increase, the first one is that effort required to measure cosine similarity will be greater and the second one is that the similarity between data will blur out and have less variation. In details, the authors generate random points with increasing number of dimensions where each dimension of a data point is given a value between 0 and 1. Then they pick one vector on random from each dimension class and calculate cosine similarity between chosen vector and the rest of the data. Then they check how the variation of values changes as the number of dimensions increases. It seems like the more dimensions there are, the smaller the variance and therefore less reliable the cosine similarity value.</p>
<div class="figure">
<img src="img/curseOfDimensionality.png" alt="curse of dimensionality, on x-axis number of dimensions, on y-axis standard deviation of similarity" />
<p class="caption">curse of dimensionality, on x-axis number of dimensions, on y-axis standard deviation of similarity</p>
</div>

<p>How to properly prepare control group in terms of quality and quantity? <!-- + check if it may be proved in the code --></p>

<p>Besides cosine similarity, there are other methods used to find the similarity between vectors.</p>
<!-- [to check https://github.com/taki0112/Vector_Similarity] -->
<!-- TODO: check in code if they make more sense -->
</div>
<div id="one-class-gender-bias-with-cosine-similarity-a-case-study" class="section level2">
<h2><span class="header-section-number">2.5</span> One class gender bias with cosine similarity: a case study</h2>
<!-- [article https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1184/reports/6835575.pdf] -->
</div>
<div id="bibliography" class="section level2">
<h2><span class="header-section-number">2.6</span> Bibliography</h2>
<ul>
<li><a href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&amp;qid=1502062931&amp;sr=8-1&amp;keywords=Neural+Network+Methods+in+Natural+Language+Processing&amp;linkCode=sl1&amp;tag=inspiredalgor-20&amp;linkId=d63df073fea3ebe2d405820570b3ff03" class="uri">https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&amp;qid=1502062931&amp;sr=8-1&amp;keywords=Neural+Network+Methods+in+Natural+Language+Processing&amp;linkCode=sl1&amp;tag=inspiredalgor-20&amp;linkId=d63df073fea3ebe2d405820570b3ff03</a></li>
</ul>
<!-- ## *** Additional -->
<!-- ### Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings" -->
<!-- [article https://arxiv.org/pdf/1607.06520.pdf] -->
<!-- Assumptions: -->
<!-- "We assume there is a set of gender neutral words $N\subseteq W$ such as flight attendant or shoes, which, by definition, are not specific to any gender." -->
<!-- TODO: check if it is useful for the analysis of black is .. -->
<!-- ### Identifying the bias subspace  -->
<!-- Interesting analysis about limits o subspace and research [source https://kawine.github.io/blog/nlp/2019/09/23/bias.html] -->
<!-- Resources to analyze: -->
<!-- 1. https://arxiv.org/pdf/2005.00965.pdf -->
<!-- 2. https://arxiv.org/pdf/2009.09435.pdf -->
<!-- 3. https://arxiv.org/pdf/1904.04047.pdf -->
<!-- 4. https://www.aclweb.org/anthology/P19-1166.pdf -->
<!-- ### How is cosine similarity entangled with analogy topic in NLP (is it??) -->
<!-- article:  https://arxiv.org/pdf/1905.09866.pdf -->
<!-- Critique of the usage of analogies as a proof for gender biases -->
<!-- TODO: check if there is other argument than that algorithm does not allow it -->


</div>
<div id="walkthrough-with-the-religion-dataset" class="section level2">
<h2><span class="header-section-number">2.7</span> Walkthrough with the religion dataset</h2>
<p>We will use the choice of protected words and stereotypical predicates used in REF. This choice is to some extent problematic, because the choice of protected classes and stereotypical predicates is not principled and is not clearly based on psychological and sociological research. However, is a decent point of departure, especially since we want to compare our method to that of REF. It is also worth keeping in mind that the method we develop takes a list of protected words and associated stereotypical predicates as an input, so the reader can fairly easy run the analysis for a different stereotypization pattern. At this stage, we will explain the method and its deployment using a dataset obtained for the religion-related protected classes. First, we load the required packages.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(ggthemes)
<span class="kw">library</span>(rethinking)
<span class="kw">library</span>(tidyverse)
<span class="kw">library</span>(ggpubr)
<span class="kw">library</span>(kableExtra)
<span class="kw">library</span>(dplyr)
<span class="kw">library</span>(ggExtra)
<span class="kw">library</span>(cowplot)</code></pre></div>

<p>Now let's load, clean a bit and inspect the head of the religion dataset we prepared DESCRIBE THE PREPARATION.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">religion &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;cosineAnalysis/datasets/religionReddit.csv&quot;</span>)[<span class="op">-</span><span class="dv">1</span>]

<span class="kw">colnames</span>(religion) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;protectedWord&quot;</span>,<span class="st">&quot;wordToCompare&quot;</span>,<span class="st">&quot;wordClass&quot;</span>,
                        <span class="st">&quot;cosineDistance&quot;</span>,<span class="st">&quot;cosineSimilarity&quot;</span>,<span class="st">&quot;connection&quot;</span>)

<span class="kw">levels</span>(religion<span class="op">$</span>wordClass) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;christian&quot;</span>,<span class="st">&quot;human&quot;</span>,<span class="st">&quot;jewish&quot;</span>,<span class="st">&quot;muslim&quot;</span>,<span class="st">&quot;neutral&quot;</span>)

<span class="kw">head</span>(religion)  <span class="op">%&gt;%</span><span class="st">  </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T,
                      <span class="dt">linesep =</span> <span class="st">&quot;&quot;</span>,  <span class="dt">escape =</span> <span class="ot">FALSE</span>, 
                      <span class="dt">caption =</span> <span class="st">&quot;Head of the religion dataset.&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">                      </span><span class="kw">kable_styling</span>(<span class="dt">latex_options=</span><span class="kw">c</span>(<span class="st">&quot;scale_down&quot;</span>))</code></pre></div>


<p>EXPLAIN THE COLUMNS</p>
<p>We will be using &quot;connection&quot; as a predictor. For technical reasons it is useful to represent an <span class="math inline">\(n\)</span>-level factor with <span class="math inline">\(n-1\)</span> binary vectors. <code>connection</code> in this case have four levels EXPLAIN WHY HUMAN AND CONTROL THIS IS CRUCIAL! We add them to the dataset.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">religion<span class="op">$</span>associated &lt;-<span class="st"> </span><span class="kw">ifelse</span>(religion<span class="op">$</span>connection <span class="op">==</span><span class="st"> &quot;associated&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
religion<span class="op">$</span>different &lt;-<span class="st"> </span><span class="kw">ifelse</span>(religion<span class="op">$</span>connection <span class="op">==</span><span class="st"> &quot;different&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
religion<span class="op">$</span>human &lt;-<span class="st"> </span><span class="kw">ifelse</span>(religion<span class="op">$</span>connection <span class="op">==</span><span class="st"> &quot;human&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)  </code></pre></div>

<p>We wrote a short script, <code>cleanDataset</code> to make this faster, so equivalently we could just say:</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;cosineAnalysis/functions/cleanDataset.R&quot;</span>)
religion &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;cosineAnalysis/datasets/religionReddit.csv&quot;</span>)[<span class="op">-</span><span class="dv">1</span>]
religion &lt;-<span class="st"> </span><span class="kw">cleanDataset</span>(religion,<span class="kw">c</span>(<span class="st">&quot;christian&quot;</span>,<span class="st">&quot;human&quot;</span>,<span class="st">&quot;jewish&quot;</span>,<span class="st">&quot;muslim&quot;</span>,<span class="st">&quot;neutral&quot;</span>))</code></pre></div>

<p>First let's take a look at the empirical distribution of distances by the connection type.</p>
<p>  </p>
<p>The impression is that EXPLAIN and EXPLAIN WHY STILL WE NEED STATISTICAL ANALYSIS</p>
<p>For now, let's focus on a single protected class, <code>muslim.</code> with which the word list associates five protected words. First, we select and plot the empirical distributions for this subset.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#levels(religion$protectedWord) you can inspect the list of protected words first</span>
muslimWords &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;imam&quot;</span>,<span class="st">&quot;islam&quot;</span>,<span class="st">&quot;mosque&quot;</span>,<span class="st">&quot;muslim&quot;</span>,<span class="st">&quot;quran&quot;</span>)
muslim &lt;-<span class="st"> </span>religion <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(protectedWord <span class="op">%in%</span><span class="st"> </span>muslimWords)
<span class="kw">ggplot</span>(muslim, <span class="kw">aes</span>(<span class="dt">x =</span>  cosineDistance, <span class="dt">fill =</span> connection))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_density</span>(<span class="dt">alpha=</span><span class="fl">0.2</span>,<span class="dt">size =</span> <span class="fl">.2</span>)<span class="op">+</span>
<span class="st">  </span><span class="kw">theme_tufte</span>()<span class="op">+</span><span class="kw">ggtitle</span>(<span class="st">&quot;Empirical distribution of distances (muslim)&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /> </p>
<p>Say we want to look at a single protected word. Now, since the dataset also contains comparison multiple control words, we randomly select only 5 from <code>none</code> and 5 from <code>human</code> control groups of those for the visualisation purposes.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">muslimClass &lt;-<span class="st"> </span>muslim <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(protectedWord <span class="op">==</span><span class="st"> &quot;muslim&quot;</span>)
neutralSample &lt;-<span class="st"> </span><span class="kw">sample_n</span>(<span class="kw">filter</span>(muslimClass,connection <span class="op">==</span><span class="st"> &quot;none&quot;</span>), <span class="dv">5</span>)
humanSample &lt;-<span class="st"> </span><span class="kw">sample_n</span>(<span class="kw">filter</span>(muslimClass,connection <span class="op">==</span><span class="st"> &quot;human&quot;</span>), <span class="dv">5</span>)
muslimVis &lt;-<span class="st"> </span>muslimClass <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(connection <span class="op">!=</span><span class="st"> &quot;none&quot;</span> <span class="op">&amp;</span><span class="st"> </span>connection <span class="op">!=</span><span class="st">&quot;human&quot;</span>)
muslimVis &lt;-<span class="st"> </span><span class="kw">rbind</span>(muslimVis,neutralSample,humanSample)

<span class="co">#we plug in our visualisation script</span>
<span class="kw">source</span>(<span class="st">&quot;cosineAnalysis/functions/visualisationTools.R&quot;</span>)
<span class="co">#two arguments: dataset and protected word</span>
<span class="kw">visualiseProtected</span>(muslimVis,<span class="st">&quot;muslim&quot;</span>)</code></pre></div>
<p><img src="_main_files/figure-html/tableMuslimActive-1.png" width="100%" style="display: block; margin: auto;" /> </p>
We will now think of cosine distance as the output variable, and will build four bayesian models. In the one we'll be ultimately concerned with, <code>cosineDistance</code> is normally distributed around the predicted mean <span class="math inline">\(\mu\)</span>, the predicted mean is a linear function of a baseline value <code>no</code>, <code>associated</code> with coefficient <span class="math inline">\(a\)</span>, <code>different</code> with coefficient <code>d</code>, and <code>human</code> with coeficient <code>h</code>.
<span class="math display">\[\begin{align}
  \mu &amp; =  \mathsf{no} + a \times \mathsf{associated}  + d \times \mathsf{different} + h \times \mathsf{human}
\end{align}\]</span>
<p>In other word, the idea is that residuals are normally distributed (with estimated standard distribution <span class="math inline">\(\sigma\)</span>) around the predicted mean, <code>no</code> is an estimated mean for connections with neutral words, <span class="math inline">\(a\)</span> is an estimated change in <code>no</code> if <code>associated</code> is activated (has value 1), <span class="math inline">\(d\)</span> is an estimated change in <code>no</code> if <code>different</code> is activated (has value 1), and <span class="math inline">\(h\)</span> is an estimated change in <code>no</code> if <code>human</code> is activated. The model parameters will have a posterior distribution obtained using either monte carlo methods or quadratic approximation (if multiple estimations are needed, this significantly improves computation time and only slighlty underestimates uncertainty) available through the <code>rethinking</code> package.</p>
Since the approach is bayesian and what we want are a posterior distribution of the parameters, we also need to start with some priors. We will use wide skeptical regularizing priors (which is the standard advice practice, as opposed to using uniform priors, which in Bayesian context is usually a bad idea). Our choice of sensible priors is this (the reader can modify and investigate the impact of their decision).
<span class="math display">\[\begin{align}
no &amp; \sim dnorm(1,.5) \\
a &amp; \sim dnorm(0,1)\\
d &amp; \sim dnorm(0,1)\\
h &amp; \sim dnorm(0,1)\\
\sigma &amp;\sim  dcauchy(0,1)
\end{align}\]</span>
<p>SAY MORE ABOUT THE PARAMETER DISTRIBUTIONS FOR THE PRIORS</p>
<p><img src="_main_files/figure-html/priorsVis-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>The simpler models either simply estimate the mean for the whole dataset and add predictors in a step-wise manner. The reason we construct them is that we first want to make sure that considering all the leves of <code>connection</code> actually has predictive value. We will construct these models for &quot;muslim&quot; and compare them. For now, we will use a computationally less demanding quandratic approximation (approximating the posterior with a normal distribution). Here are the models:</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rethinking)
muslim.null  &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu <span class="op">~</span><span class="st">  </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    sigma  <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> muslim,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">mu =</span> <span class="dv">1</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)

muslim.a &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>associated,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    a <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> muslim,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)


muslim.ad &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>associated  <span class="op">+</span><span class="st"> </span>d <span class="op">*</span><span class="st"> </span>different,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    a <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    d <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> muslim,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">d =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)

muslim.adh &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>associated  <span class="op">+</span><span class="st"> </span>d <span class="op">*</span><span class="st"> </span>different <span class="op">+</span><span class="st"> </span>h <span class="op">*</span><span class="st"> </span>human,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    a <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    d <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    h <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> muslim,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">d =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)</code></pre></div>

<p>We first look at the parameters provided by the models (we need to define a function that converts the precis to the data frame format):</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">precisToDf &lt;-<span class="st"> </span><span class="cf">function</span> (model){
  modelDf &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">precis</span>(model)[<span class="dv">1</span>],<span class="kw">precis</span>(model)[<span class="dv">2</span>],
                   <span class="kw">precis</span>(model)[<span class="dv">3</span>],<span class="kw">precis</span>(model)[<span class="dv">4</span>])
  <span class="kw">return</span>(modelDf)
}

<span class="kw">precisToDf</span>(muslim.null) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T,
      <span class="dt">caption =</span> <span class="st">&quot;Precis of the baseline model.&quot;</span>)</code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">precisToDf</span>(muslim.a) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T, 
      <span class="dt">caption =</span> <span class="st">&quot;Precis of a model with associated as a predictor.&quot;</span>)</code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">precisToDf</span>(muslim.ad) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T, 
      <span class="dt">caption =</span> <span class="st">&quot;Precis of a model with associated and different as  predictors.&quot;</span>)</code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">precisToDf</span>(muslim.adh) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T,
      <span class="dt">caption =</span> <span class="st">&quot;Precis of the full model.&quot;</span>)</code></pre></div>


<p>EXPLAIN HOW TO READ THE COEFFICIENTS AND THE 89 PERCENTILE INTERVALS</p>
<p>Now we can investigate how the models perform (rounding the results to two digits)</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">compare</span>( muslim.null , muslim.a , muslim.ad, muslim.adh ),<span class="dv">2</span>)</code></pre></div>
<pre><code>##                WAIC    SE dWAIC   dSE pWAIC weight
## muslim.adh  -771.64 57.00  0.00    NA  5.79      1
## muslim.a    -741.51 56.83 30.13 10.83  3.66      0
## muslim.ad   -739.44 56.81 32.21 10.78  4.88      0
## muslim.null -728.03 57.59 43.61 15.23  1.97      0</code></pre>

<p>Clearly, the full model is much better and should be given the full weight. Let's double check if a similar decision should be made if we use the whole religion dataset.</p>
<p> </p>
<pre><code>##                   WAIC     SE  dWAIC   dSE pWAIC weight
## religion.adh  -2249.08 101.25   0.00    NA  5.48      1
## religion.ad   -2100.89 100.88 148.19 23.45  4.74      0
## religion.a    -2097.78 100.82 151.30 23.92  3.68      0
## religion.null -2088.42 101.08 160.66 25.40  1.96      0</code></pre>

<p>Quite crucially, note that the Widely Applicable Information Criterion (WAIC) is lowest for the full model and that the weigth assigned to the null model makes in negligible. This indicates that the predictor variables in the full model indeed have predictive power and that in what follows we can ignore the other models.</p>
<p>Now, let's use <code>STAN</code> to build a markov chain monte carlo model with the same parameters for the muslim subsset. For us the practical difference is that the method is computationally a bit more demanding, and that because the model construction avoids quadratic approximation the resulting uncertainty is realistically slightly larger.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">buildtools.check =</span> <span class="cf">function</span>(action) <span class="ot">TRUE</span> ) <span class="co">#removes install pop-up request</span>
muslim.adh.st &lt;-<span class="st"> </span><span class="kw">map2stan</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>associated  <span class="op">+</span><span class="st"> </span>d <span class="op">*</span><span class="st"> </span>different <span class="op">+</span><span class="st"> </span>h <span class="op">*</span><span class="st"> </span>human,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    a <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    d <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    h <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> muslim,
  <span class="dt">chains=</span><span class="dv">2</span> , <span class="dt">iter=</span><span class="dv">4000</span> , <span class="dt">warmup=</span><span class="dv">1000</span>,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">a =</span> <span class="dv">0</span>, <span class="dt">d =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)


<span class="kw">precisToDf</span>(<span class="kw">precis</span>(muslim.adh.st))<span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T,
      <span class="dt">caption =</span> <span class="st">&quot;Precis of the full mcmc model.&quot;</span>)</code></pre></div>

<p>EXPLAIN WHAT'S GOING ON IN THE TABLES, DON'T FORGET IT'S HPDI not percentiles now!</p>
<p>We can now take a look at the diagnostic plots, displaying chains for the parameters of interest (what we're looking for is stationarity and a random-walk-like look):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">source</span>(<span class="st">&quot;cosineAnalysis/functions/plotChains.R&quot;</span>)
<span class="kw">plotChains</span>(muslim.adh.st)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-14-1.png" width="100%" style="display: block; margin: auto;" /> </p>
<p>We can also visualise the coefficients generated by the procedure:</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">coeftab</span>(muslim.adh.st))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-15-1.png" width="100%" style="display: block; margin: auto;" /> </p>
<p>Next, we investigate the uncertainty involved, by working with the samples extracted from the model.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">samples &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">extract.samples</span> (muslim.adh.st, <span class="dt">n =</span> <span class="fl">1e4</span> ))
<span class="kw">head</span>(samples, <span class="dt">n =</span> <span class="dv">6</span> )  <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">kable</span>(<span class="dt">format =</span> <span class="st">&quot;latex&quot;</span>,<span class="dt">booktabs=</span>T, <span class="dt">linesep =</span> <span class="st">&quot;&quot;</span>,
      <span class="dt">caption =</span> <span class="st">&quot;Head of samples extracted from the full mcmc model.&quot;</span>)</code></pre></div>


<p>EXPLAIN THIS</p>
<p>Now for each datapoint in the sample we calculate the predicted <span class="math inline">\(\mu\)</span> for the four categories.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu.no &lt;-<span class="st"> </span>samples<span class="op">$</span>no 
mu.a &lt;-<span class="st"> </span>samples<span class="op">$</span>no <span class="op">+</span><span class="st"> </span>samples<span class="op">$</span>a
mu.d &lt;-<span class="st"> </span>samples<span class="op">$</span>no <span class="op">+</span><span class="st"> </span>samples<span class="op">$</span>d
mu.h &lt;-<span class="st"> </span>samples<span class="op">$</span>no <span class="op">+</span><span class="st"> </span>samples<span class="op">$</span>h</code></pre></div>

<p>Next, we can look at the sample posterior means for the categories, first let's inspect them visually.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mus &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">none =</span> mu.no, <span class="dt">associated =</span> mu.a,
                  <span class="dt">different =</span> mu.d, <span class="dt">human =</span> mu.h)
musLong &lt;-<span class="st">  </span><span class="kw">gather</span>(mus, connection, mu, <span class="kw">c</span>(none,associated,different, human),
                   <span class="dt">factor_key=</span><span class="ot">TRUE</span>)

musPlot &lt;-<span class="st"> </span><span class="kw">ggplot</span>(musLong, <span class="kw">aes</span>(<span class="dt">x=</span>connection,<span class="dt">y=</span>mu, <span class="dt">color =</span> connection,
                               <span class="dt">fill=</span> connection))<span class="op">+</span>
<span class="st">  </span><span class="kw">geom_jitter</span>(<span class="dt">alpha=</span><span class="fl">0.07</span>)<span class="op">+</span><span class="kw">theme_tufte</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">color =</span> <span class="kw">guide_legend</span>(<span class="dt">override.aes =</span> <span class="kw">list</span>(<span class="dt">alpha =</span> <span class="dv">1</span>)))

<span class="kw">ggMarginal</span>(musPlot, <span class="dt">type =</span> <span class="st">&quot;density&quot;</span>, <span class="dt">groupColour =</span> <span class="ot">TRUE</span>)</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-18-1.png" width="100%" style="display: block; margin: auto;" /> </p>
<p>Then, let's calculate their means and HPDIs.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data.frame</span>(<span class="kw">precis</span>(<span class="kw">data.frame</span>(mu.no,mu.a,mu.d, mu.h)))[,<span class="op">-</span><span class="dv">5</span>]</code></pre></div>
<pre><code>##            mean          sd     X5.5.    X94.5.
## mu.no 0.9416495 0.004981136 0.9336039 0.9495987
## mu.a  0.7585600 0.042536604 0.6890276 0.8253921
## mu.d  0.9221943 0.032060355 0.8707138 0.9722415
## mu.h  0.8309126 0.018369462 0.8014352 0.8602604</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hpdis &lt;-<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">HPDI</span>( mu.no ),<span class="kw">HPDI</span>( mu.a ),
               <span class="kw">HPDI</span>( mu.d ), <span class="kw">HPDI</span>( mu.h ))
<span class="kw">rownames</span>(hpdis) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;none&quot;</span>,<span class="st">&quot;associated&quot;</span>,<span class="st">&quot;different&quot;</span>,<span class="st">&quot;human&quot;</span>)
hpdis </code></pre></div>
<pre><code>##                |0.89     0.89|
## none       0.9337161 0.9496828
## associated 0.6896347 0.8256707
## different  0.8722726 0.9735840
## human      0.8004882 0.8591354</code></pre>

<p>In our summary visualisation later on, we will represent the information in a slightly different manner. First we put the means in a single dataframe, and then plot the means of means as points and their corresponding HPDIs as lines. For a single protected class this isn't terribly exciting, but it will come useful in a moment.</p>

<p>Now, let's generalize and build a model for all the protected words in the religion class and extract parameters from the model both for the full dataset and each protected word. We'll use a script we wrote for this purpose, feel free to re-run, but be aware that the computation takes some time. For this reason, here we just show you the higher-level code, but in fact use a previously obtained dataframe with the output as we proceed. Here we provide the generating code, but it's commented out, and we load and use the obtained table instead. We'll compare it to the parameters obtained fromt he database based on the Google word2vec embeddings.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#options(buildtools.check = function(action) TRUE )</span>
<span class="co">#source(&quot;cosineAnalysis/functions/cleanDataset.R&quot;)</span>
<span class="co">#source(&quot;cosineAnalysis/functions/evaluateProtected.R&quot;)</span>
<span class="co">#religion &lt;- read.csv(&quot;cosineAnalysis/datasets/religionReddit.csv&quot;)[-1]</span>
<span class="co">#religion &lt;- cleanDataset(religion,c(&quot;christian&quot;,&quot;human&quot;,</span>
              <span class="co">#&quot;jewish&quot;,&quot;muslim&quot;,&quot;neutral&quot;))</span>
<span class="co">#now built it for all protected classes as well</span>
<span class="co">#religionTable &lt;- parTable(religion)</span>
<span class="co">#save(religionTable,file = &quot;cosineAnalysis/datasets/religionTable.RData&quot;) </span>
<span class="kw">load</span>(<span class="st">&quot;cosineAnalysis/datasets/religionTable.RData&quot;</span>)
<span class="kw">source</span>(<span class="st">&quot;cosineAnalysis/functions/visualiseStats.R&quot;</span>)
<span class="kw">visualiseTable</span>(religionTable)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:religionOriginalStats"></span>
<img src="_main_files/figure-html/religionOriginalStats-1.png" alt="Posterior probabilities for the Reddit-based religion dataset." width="100%" />
<p class="caption">
Figure 2.1: Posterior probabilities for the Reddit-based religion dataset.
</p>
</div>

<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#options(buildtools.check = function(action) TRUE )</span>
<span class="co">#source(&quot;cosineAnalysis/functions/cleanDataset.R&quot;)</span>
<span class="co">#source(&quot;cosineAnalysis/functions/evaluateProtected.R&quot;)</span>
<span class="co">#religionGoogle &lt;- read.csv(&quot;cosineAnalysis/datasets/religionGoogle.csv&quot;)[-1]</span>
<span class="co">#religionGoogle &lt;- cleanDataset(religionGoogle,c(&quot;christian&quot;,&quot;human&quot;,</span>
<span class="co">#&quot;jewish&quot;,&quot;muslim&quot;,&quot;neutral&quot;))</span>
<span class="co">#now built it for all protected classes as well</span>
<span class="co">#religionGoogleTable &lt;- parTable(religionGoogle)</span>
<span class="co">#save(religionGoogleTable,file = &quot;cosineAnalysis/datasets/religionGoogleTable.RData&quot;) </span>
<span class="kw">load</span>(<span class="st">&quot;cosineAnalysis/datasets/religionGoogleTable.RData&quot;</span>)
<span class="kw">source</span>(<span class="st">&quot;cosineAnalysis/functions/visualiseStats.R&quot;</span>)
<span class="kw">visualiseTable</span>(religionGoogleTable)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:religionGoogleStats"></span><span id="fig:genderOriginalStats"></span><span id="fig:genderGoogleStats"></span><span id="fig:raceRedditStats"></span><span id="fig:raceGoogleStats"></span>
<img src="_main_files/figure-html/religionGoogleStats-1.png" alt="Posterior probabilities for the Google-based religion dataset." width="100%" />
<p class="caption">
Figure 2.2: Posterior probabilities for the Google-based religion dataset.
</p>
</div>

<p>Here are analogous results for the gender and race datasets:</p>
 <img src="_main_files/figure-html/genderOriginalStats-1.png" alt="Posterior probabilities for the Reddit-based gender dataset." width="100%" />
<p class="caption">
Figure 2.3: Posterior probabilities for the Reddit-based gender dataset.
</p>
</div>
 <img src="_main_files/figure-html/genderGoogleStats-1.png" alt="Posterior probabilities for the Google-based gender dataset." width="100%" />
<p class="caption">
Figure 2.4: Posterior probabilities for the Google-based gender dataset.
</p>
</div>
 <img src="_main_files/figure-html/raceRedditStats-1.png" alt="Posterior probabilities for the Reddit-based race dataset." width="100%" />
<p class="caption">
Figure 2.5: Posterior probabilities for the Reddit-based race dataset.
</p>
</div>
 <img src="_main_files/figure-html/raceGoogleStats-1.png" alt="Posterior probabilities for the Google-based race dataset." width="100%" />
<p class="caption">
Figure 2.6: Posterior probabilities for the Google-based race dataset.
</p>
</div>
</div>
<div id="control-groups-in-the-protectedword-position" class="section level2">
<h2><span class="header-section-number">2.8</span> Control groups in the protectedWord position</h2>
<p>Motivate and EXPLAIN the experiment.</p>
<p>First, cleaning is a bit different (and we need to introduce more dummy variables).  </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">controlReddit &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;cosineAnalysis/datasets/controlReddit.csv&quot;</span>)[<span class="op">-</span><span class="dv">1</span>]
controlGoogle &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;cosineAnalysis/datasets/controlGoogle.csv&quot;</span>)[<span class="op">-</span><span class="dv">1</span>]


controlRedditClean &lt;-<span class="st"> </span><span class="kw">cleanDataset</span>(controlReddit, <span class="kw">c</span>(<span class="st">&quot;asian&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;caucasian&quot;</span>,
                      <span class="st">&quot;christian&quot;</span>, <span class="st">&quot;human&quot;</span>, <span class="st">&quot;jew&quot;</span>, <span class="st">&quot;man&quot;</span>, <span class="st">&quot;muslim&quot;</span>, <span class="st">&quot;neutral&quot;</span>, <span class="st">&quot;woman&quot;</span>))

controlGoogleClean &lt;-<span class="st"> </span><span class="kw">cleanDataset</span>(controlGoogle, <span class="kw">c</span>(<span class="st">&quot;asian&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;caucasian&quot;</span>,
                    <span class="st">&quot;christian&quot;</span>, <span class="st">&quot;human&quot;</span>, <span class="st">&quot;jew&quot;</span>, <span class="st">&quot;man&quot;</span>, <span class="st">&quot;muslim&quot;</span>, <span class="st">&quot;neutral&quot;</span>, <span class="st">&quot;woman&quot;</span>))

humans &lt;-<span class="st"> </span>controlRedditClean <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">filter</span>(connection <span class="op">==</span><span class="st">&quot;human&quot;</span>)
humanWords &lt;-<span class="st"> </span><span class="kw">droplevels</span>(<span class="kw">unique</span>(humans<span class="op">$</span>protectedWord))


reddit &lt;-<span class="st"> </span>controlRedditClean[,<span class="dv">1</span><span class="op">:</span>(<span class="kw">length</span>(controlRedditClean)<span class="op">-</span><span class="dv">4</span>)]
google &lt;-<span class="st"> </span>controlGoogleClean[,<span class="dv">1</span><span class="op">:</span>(<span class="kw">length</span>(controlGoogleClean)<span class="op">-</span><span class="dv">4</span>)]


reddit<span class="op">$</span>humanProtected &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>protectedWord <span class="op">%in%</span><span class="st"> </span>humanWords, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>humanProtected &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>protectedWord <span class="op">%in%</span><span class="st"> </span>humanWords, <span class="dv">1</span>, <span class="dv">0</span>)

reddit<span class="op">$</span>humanPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;human&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>activePredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">!=</span><span class="st"> &quot;neutral&quot;</span> <span class="op">&amp;</span>
<span class="st">                                   </span>reddit<span class="op">$</span>wordClass <span class="op">!=</span><span class="st"> &quot;human&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>asianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;asian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>blackPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;black&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>caucasianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;caucasian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>christianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;christian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>jewPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;jew&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>manPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;man&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>muslimPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;muslim&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
reddit<span class="op">$</span>womanPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(reddit<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;woman&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)


google<span class="op">$</span>humanPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;human&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>activePredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">!=</span><span class="st"> &quot;neutral&quot;</span> <span class="op">&amp;</span>
<span class="st">                                   </span>google<span class="op">$</span>wordClass <span class="op">!=</span><span class="st"> &quot;human&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>asianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;asian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>blackPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;black&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>caucasianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;caucasian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>christianPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;christian&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>jewPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;jew&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>manPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;man&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>muslimPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;muslim&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)
google<span class="op">$</span>womanPredicate &lt;-<span class="st"> </span><span class="kw">ifelse</span>(google<span class="op">$</span>wordClass <span class="op">==</span><span class="st"> &quot;woman&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)</code></pre></div>

<p>Now we build the four models EXPLAIN.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">controlReddit.model &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map2stan</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>H <span class="op">*</span><span class="st"> </span>humanProtected  <span class="op">+</span><span class="st"> </span>hp <span class="op">*</span><span class="st"> </span>humanPredicate <span class="op">+</span><span class="st"> </span>ac <span class="op">*</span><span class="st"> </span>activePredicate,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,<span class="dv">1</span>),
    H <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    hp <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    ac <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,.<span class="dv">5</span>)
  ),
  <span class="dt">data =</span> reddit,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">H =</span> <span class="dv">0</span>, <span class="dt">hp =</span> <span class="dv">0</span>, <span class="dt">ac =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)

controlGoogle.model &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map2stan</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>H <span class="op">*</span><span class="st"> </span>humanProtected  <span class="op">+</span><span class="st"> </span>hp <span class="op">*</span><span class="st"> </span>humanPredicate <span class="op">+</span><span class="st"> </span>ac <span class="op">*</span><span class="st"> </span>activePredicate,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,<span class="dv">1</span>),
    H <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    hp <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    ac <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,.<span class="dv">5</span>)
  ),
  <span class="dt">data =</span> google,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">H =</span> <span class="dv">0</span>, <span class="dt">hp =</span> <span class="dv">0</span>, <span class="dt">ac =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)

controlReddit.fullModel &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map2stan</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>H <span class="op">*</span><span class="st"> </span>humanProtected  <span class="op">+</span><span class="st"> </span>hp <span class="op">*</span><span class="st"> </span>humanPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>asian <span class="op">*</span><span class="st"> </span>asianPredicate <span class="op">+</span><span class="st"> </span>bl <span class="op">*</span><span class="st"> </span>blackPredicate <span class="op">+</span>
<span class="st">      </span>ca <span class="op">*</span><span class="st"> </span>caucasianPredicate <span class="op">+</span><span class="st"> </span>chr <span class="op">*</span><span class="st"> </span>christianPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>wo <span class="op">*</span><span class="st"> </span>womanPredicate <span class="op">+</span><span class="st"> </span>j <span class="op">*</span><span class="st"> </span>jewPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>man <span class="op">*</span><span class="st"> </span>manPredicate <span class="op">+</span><span class="st"> </span>mus <span class="op">*</span><span class="st"> </span>muslimPredicate,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    H <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    hp <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    asian <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    bl <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    ca <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    chr <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    wo <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    j <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    man <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    mus <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> reddit,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">H =</span> <span class="dv">0</span>, <span class="dt">hp =</span> <span class="dv">0</span>, <span class="dt">asian =</span> <span class="dv">0</span>, <span class="dt">bl =</span> <span class="dv">0</span>, <span class="dt">ca =</span> <span class="dv">0</span>, 
             <span class="dt">chr =</span> <span class="dv">0</span>, <span class="dt">wo =</span> <span class="dv">0</span>, <span class="dt">j =</span> <span class="dv">0</span>, <span class="dt">man =</span> <span class="dv">0</span>, <span class="dt">mus =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)

controlGoogle.fullModel &lt;-<span class="st"> </span>rethinking<span class="op">::</span><span class="kw">map2stan</span>(
  <span class="kw">alist</span>(
    cosineDistance <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(mu,sigma),
    mu &lt;-<span class="st"> </span>no <span class="op">+</span><span class="st"> </span>H <span class="op">*</span><span class="st"> </span>humanProtected  <span class="op">+</span><span class="st"> </span>hp <span class="op">*</span><span class="st"> </span>humanPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>asian <span class="op">*</span><span class="st"> </span>asianPredicate <span class="op">+</span><span class="st">    </span>bl <span class="op">*</span><span class="st"> </span>blackPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>ca <span class="op">*</span><span class="st"> </span>caucasianPredicate <span class="op">+</span><span class="st"> </span>chr <span class="op">*</span><span class="st"> </span>christianPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>wo <span class="op">*</span><span class="st"> </span>womanPredicate <span class="op">+</span><span class="st"> </span>j <span class="op">*</span><span class="st"> </span>jewPredicate <span class="op">+</span><span class="st"> </span>
<span class="st">      </span>man <span class="op">*</span><span class="st"> </span>manPredicate <span class="op">+</span><span class="st"> </span>mus <span class="op">*</span><span class="st"> </span>muslimPredicate,
    no <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">1</span>,.<span class="dv">5</span>),
    H <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    hp <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    asian <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    bl <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    ca <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    chr <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    wo <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    j <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    man <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    mus <span class="op">~</span><span class="st"> </span><span class="kw">dnorm</span>(<span class="dv">0</span>,<span class="dv">1</span>),
    sigma <span class="op">~</span><span class="st"> </span><span class="kw">dcauchy</span>(<span class="dv">0</span>,<span class="dv">1</span>)
  ),
  <span class="dt">data =</span> google,
  <span class="dt">start=</span><span class="kw">list</span>(<span class="dt">no =</span> <span class="dv">1</span>, <span class="dt">H =</span> <span class="dv">0</span>, <span class="dt">hp =</span> <span class="dv">0</span>, <span class="dt">asian =</span> <span class="dv">0</span>, <span class="dt">bl =</span> <span class="dv">0</span>, <span class="dt">ca =</span> <span class="dv">0</span>, 
             <span class="dt">chr =</span> <span class="dv">0</span>, <span class="dt">wo =</span> <span class="dv">0</span>, <span class="dt">j =</span> <span class="dv">0</span>, <span class="dt">man =</span> <span class="dv">0</span>, <span class="dt">mus =</span> <span class="dv">0</span>, <span class="dt">sigma=</span> <span class="fl">.3</span>)
)</code></pre></div>
<p>Next, compare the models pair-wise for the two datasets. EXPLAIN.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">compare</span>(controlReddit.fullModel,controlReddit.model),<span class="dv">3</span>)</code></pre></div>
<pre><code>##                              WAIC      SE  dWAIC    dSE  pWAIC weight
## controlReddit.fullModel -1953.116 106.245  0.000     NA 12.705      1
## controlReddit.model     -1915.684 107.055 37.432 14.153  4.761      0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">compare</span>(controlGoogle.fullModel,controlGoogle.model),<span class="dv">3</span>)</code></pre></div>
<pre><code>##                              WAIC      SE  dWAIC   dSE  pWAIC weight
## controlGoogle.fullModel -11921.46 195.562  0.000    NA 16.309  0.995
## controlGoogle.model     -11910.87 195.237 10.591 11.62  7.951  0.005</code></pre>

<p>In both cases the full model does better. So let's take a look at the full model coefficients.</p>
<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">precis</span>(controlReddit.fullModel),<span class="dv">3</span>)</code></pre></div>
<pre><code>##         mean    sd   5.5%  94.5%    n_eff Rhat4
## no     0.901 0.004  0.894  0.907 1233.511 0.999
## H      0.011 0.006  0.002  0.020 1480.778 0.999
## hp    -0.002 0.011 -0.021  0.016 1839.619 0.999
## asian -0.041 0.025 -0.081 -0.003 1750.615 1.000
## bl    -0.024 0.025 -0.062  0.017 1458.317 1.000
## ca    -0.062 0.022 -0.098 -0.027 1363.198 1.000
## chr    0.132 0.032  0.082  0.181 1895.015 0.999
## wo    -0.072 0.016 -0.098 -0.045 1622.760 1.001
## j      0.022 0.027 -0.023  0.065 1490.856 1.001
## man   -0.033 0.017 -0.062 -0.005 1599.382 1.000
## mus    0.065 0.029  0.021  0.111 1758.201 0.999
## sigma  0.200 0.002  0.197  0.203 1930.667 0.999</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">round</span>(<span class="kw">precis</span>(controlGoogle.fullModel),<span class="dv">3</span>)</code></pre></div>
<pre><code>##         mean    sd   5.5%  94.5%    n_eff Rhat4
## no     0.934 0.002  0.932  0.937 1670.607 0.999
## H     -0.005 0.002 -0.009 -0.002 1873.084 1.000
## hp    -0.009 0.004 -0.016 -0.002 1405.339 1.000
## asian -0.023 0.011 -0.040 -0.006  910.808 1.000
## bl    -0.021 0.009 -0.037 -0.007 1366.866 1.001
## ca     0.009 0.008 -0.004  0.022  926.916 0.999
## chr    0.015 0.012 -0.004  0.034 1134.998 0.999
## wo    -0.026 0.006 -0.036 -0.016  975.250 0.999
## j     -0.014 0.010 -0.030  0.002 1006.316 1.003
## man    0.004 0.006 -0.005  0.012 1026.982 0.999
## mus   -0.017 0.010 -0.033 -0.001  980.371 1.001
## sigma  0.077 0.001  0.076  0.078 1009.715 0.999</code></pre>

<p> </p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">r &lt;-<span class="st"> </span>controlReddit.fullModel
g &lt;-<span class="st"> </span>controlGoogle.fullModel
<span class="kw">plot</span>(<span class="kw">coeftab</span>(r))
<span class="kw">plot</span>(<span class="kw">coeftab</span>(g))</code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-24-1.png" width="80%" style="display: block; margin: auto;" /><img src="_main_files/figure-html/unnamed-chunk-24-2.png" width="80%" style="display: block; margin: auto;" /> </p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="understanding-the-method-of-gender-bias-detection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
