<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Limitations of the approach | Conceptual and methodological problems with bias detection and avoidance in natural language processing</title>
  <meta name="description" content="3 Limitations of the approach | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Limitations of the approach | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Limitations of the approach | Conceptual and methodological problems with bias detection and avoidance in natural language processing" />
  
  
  

<meta name="author" content="Alicja Dobrzeniecka" />


<meta name="date" content="2021-06-08" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="cosine-similarity-and-bias-detection.html"/>
<link rel="next" href="walkthrough-with-the-religion-dataset.html"/>
<script src="book_assets/jquery-2.2.3/jquery.min.js"></script>
<link href="book_assets/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="book_assets/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html"><i class="fa fa-check"></i><b>2</b> Cosine similarity and bias detection</a><ul>
<li class="chapter" data-level="2.1" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#word-embeddings"><i class="fa fa-check"></i><b>2.1</b> Word embeddings</a></li>
<li class="chapter" data-level="2.2" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-similarity-and-distance"><i class="fa fa-check"></i><b>2.2</b> Cosine similarity and distance</a></li>
<li class="chapter" data-level="2.3" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-distance-in-a-one-class-bias-detection"><i class="fa fa-check"></i><b>2.3</b> Cosine distance in a one-class bias detection</a></li>
<li class="chapter" data-level="2.4" data-path="cosine-similarity-and-bias-detection.html"><a href="cosine-similarity-and-bias-detection.html#cosine-distance-in-a-multi-class-bias-detection"><i class="fa fa-check"></i><b>2.4</b> Cosine distance in a multi-class bias detection</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html"><i class="fa fa-check"></i><b>3</b> Limitations of the approach</a><ul>
<li class="chapter" data-level="3.1" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html#selection-and-number-of-attributes"><i class="fa fa-check"></i><b>3.1</b> Selection and number of attributes</a></li>
<li class="chapter" data-level="3.2" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html#no-control-groups"><i class="fa fa-check"></i><b>3.2</b> No control groups</a></li>
<li class="chapter" data-level="3.3" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html#means-of-means"><i class="fa fa-check"></i><b>3.3</b> Means of means</a></li>
<li class="chapter" data-level="3.4" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html#interpretability-issues"><i class="fa fa-check"></i><b>3.4</b> Interpretability issues</a></li>
<li class="chapter" data-level="3.5" data-path="limitations-of-the-approach.html"><a href="limitations-of-the-approach.html#the-curse-of-dimensionality"><i class="fa fa-check"></i><b>3.5</b> The curse of dimensionality</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html"><i class="fa fa-check"></i><b>4</b> Walkthrough with the religion dataset</a><ul>
<li class="chapter" data-level="4.1" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#loading-and-understanding-the-dataset"><i class="fa fa-check"></i><b>4.1</b> Loading and understanding the dataset</a></li>
<li class="chapter" data-level="4.2" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#first-look-at-the-empirical-distributions"><i class="fa fa-check"></i><b>4.2</b> First look at the empirical distributions</a></li>
<li class="chapter" data-level="4.3" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#looking-at-the-islam-related-words"><i class="fa fa-check"></i><b>4.3</b> Looking at the islam-related words</a></li>
<li class="chapter" data-level="4.4" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#bayesian-model-structure-and-assumptions"><i class="fa fa-check"></i><b>4.4</b> Bayesian model structure and assumptions</a></li>
<li class="chapter" data-level="4.5" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#choosing-predictors"><i class="fa fa-check"></i><b>4.5</b> Choosing predictors</a></li>
<li class="chapter" data-level="4.6" data-path="walkthrough-with-the-religion-dataset.html"><a href="walkthrough-with-the-religion-dataset.html#dataset-level-coefficients"><i class="fa fa-check"></i><b>4.6</b> Dataset-level coefficients</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html"><i class="fa fa-check"></i><b>5</b> Protected-word level analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html#model-structure-and-assumptions"><i class="fa fa-check"></i><b>5.1</b> Model structure and assumptions</a></li>
<li class="chapter" data-level="5.2" data-path="protected-word-level-analysis.html"><a href="protected-word-level-analysis.html#protected-classes-in-reddit-and-google-embeddings"><i class="fa fa-check"></i><b>5.2</b> Protected classes in Reddit and Google embeddings</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html"><i class="fa fa-check"></i><b>6</b> The role of debiasing</a><ul>
<li class="chapter" data-level="6.1" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html#dataset-level-coefficients-after-debiasing"><i class="fa fa-check"></i><b>6.1</b> Dataset-level coefficients after debiasing</a></li>
<li class="chapter" data-level="6.2" data-path="the-role-of-debiasing.html"><a href="the-role-of-debiasing.html#protected-classes-after-debiasing"><i class="fa fa-check"></i><b>6.2</b> Protected classes after debiasing</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Conceptual and methodological problems with bias detection and avoidance in natural language processing</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="limitations-of-the-approach" class="section level1">
<h1><span class="header-section-number">3</span> Limitations of the approach</h1>
<div id="selection-and-number-of-attributes" class="section level2">
<h2><span class="header-section-number">3.1</span> Selection and number of attributes</h2>
<!-- TODO: Universal ways to decide on the quality and type of attributes so that the research is significant? -->
<p>The attributes are taken from different sources, therefore there is no one justification for their choice.</p>
<!-- TODO: Universal ways to decide on the number of attributes so that proving bias is significant? -->
<!-- https://cs.stanford.edu/people/wmorgan/sigtest.pdf -->
<p>In the article there is no mention of methodology for deciding on the number of attributes necessary to prove a hypothesis on the given size of dataset. There are however some ways to estimate how many samples we need to make sure that the result is significant.</p>
</div>
<div id="no-control-groups" class="section level2">
<h2><span class="header-section-number">3.2</span> No control groups</h2>
<p> </p>
<p>How to properly prepare control group in terms of quality and quantity?</p>
</div>
<div id="means-of-means" class="section level2">
<h2><span class="header-section-number">3.3</span> Means of means</h2>
<p>The authors use the mean average cosine similarity to check on multi-class similarity between protected word and harmful stereotypes. Details of the process are described as follows.</p>
<p>When one calculates individual cosine similarity between some chosen word like &quot;christianity&quot; and neutral attributes that are supposed to be biased are actually often having very small and negative similarity value. As a result the measurement cosine distance is for such pair of words greater than 1. The similarity interpretation for that would suggest that the words are not similar as their cosine distance is so enormous.</p>
<p>Doing a mean hides this issue and as there are pairs having negative and small similarities and there are those that have similarity equal to 0.5, the resulting calculation seems to be in norm. Additionally in such method the uncertainty interval is also not included which makes it even more difficult to give reasonable interpretations of the results. In the original paper words from all three religions were checked with all of the stereotypes which means that there was no distinction on whether the stereotype is associated with given religion or with the other. Not all of the stereotypical words should be considered as harmful for all of the religions. In our research we distinguished between associated stereotypes, not associated (stereotype is valid but for different religion than currently checked) and none (meaning neutral words). </p>
</div>
<div id="interpretability-issues" class="section level2">
<h2><span class="header-section-number">3.4</span> Interpretability issues</h2>
<p>Assuming for a moment that the value of multi-class cosine distance is correct, one may question the results' interpretation. In [Black is to Criminal as Caucasian is to Police:Detecting and Removing Multiclass Bias in Word Embeddings] Table 2, there are summarized the averages of cosine distance per group (gender, race, religion). I would like to focus now on analyzing the values relating to religious biases. Here is the fragment of table that refers to that:</p>
<table>
<thead>
<tr class="header">
<th>Religion Debiasing</th>
<th>MAC</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Biased</td>
<td>0.859</td>
</tr>
<tr class="even">
<td>Hard Debiased</td>
<td>0.934</td>
</tr>
<tr class="odd">
<td>Soft Debiased (<span class="math inline">\(\lambda\)</span> = 0.2)</td>
<td>0.894</td>
</tr>
</tbody>
</table>
<p>MAC stand for mean average cosine similarity although in reality the values of cosine distance are there stored. What may attract attention is the fact that the value of cosine distance in &quot;Biased&quot; category is already quite high even before &quot;debiasing&quot;. High cosine distance indicates low cosine similarity between values. One could think that average cosine similarity equal to approximately 0.141 is not significant enough to consider it as biased. However the authors aim to mitigate &quot;biases&quot; in vectors with such great distance to make it even larger. Methodologically there is a question on what bases this small similarity is still considered as a proof of bias presence.</p>
<p>This is in general the problem of scale and the lack of universal intervals that could be applied to know whether the cosine similarity is high enough to considered given words more similar that if we chose them on random. In other metrics like Pearson coefficient we are provided with artificial interval that tries to make the interpretations more objective with a point of reference. It seems like similar practic could be beneficial for metrics aimed at measuring similarity between words.</p>
</div>
<div id="the-curse-of-dimensionality" class="section level2">
<h2><span class="header-section-number">3.5</span> The curse of dimensionality</h2>
<!-- TODO: -->
<!-- 1. https://www.cs.princeton.edu/courses/archive/fall13/cos521/lecnotes/lec11.pdf -->
<!-- 2. https://stats.stackexchange.com/questions/341535/curse-of-dimensionality-does-cosine-similarity-work-better-and-if-so-why -->
<!-- 3. https://stats.stackexchange.com/questions/21547/distance-metric-and-curse-of-dimensions -->
<!-- + check if it may be proved in the code -->
<p>Curse of dimensionality may take place when there is an increase in volume of data that results in adding extra dimensions to the Euclidean space. According to the article &quot;<a href="https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/" class="uri">https://analyticsindiamag.com/curse-of-dimensionality-and-what-beginners-should-do-to-overcome-it/</a>&quot; as the number of features increases, it may be harder and harder to get useful information from the data with the usage of available algorithms. One may notice that more data should contribute to greater amount of information but more information also means greater risk of noise and distractions in data. At the same time, many times modern solutions are adapted to smaller dimensions and their results in higher ones are not intuitive or may be prone to be mistaken.</p>
<p>Using cosine similarity in high dimensions in word embeddings may also be prone to the curse of dimensionality. According to this article &quot;<a href="https://www.researchgate.net/publication/327498046_The_Curse_of_Dimensionality_Inside_Out" class="uri">https://www.researchgate.net/publication/327498046_The_Curse_of_Dimensionality_Inside_Out</a>&quot; there are reasons to consider this phenomenon when searching for word similarities in higher dimensions.</p>
<!-- Add more information on the experiment -->
<p>In the article an experiment is conducted that aims at showing how the similarity values and variation change as the number of dimensions increases. The hypothesis made in the paper states that two things will happen as the number of dimensions increase, the first one is that effort required to measure cosine similarity will be greater and the second one is that the similarity between data will blur out and have less variation. In details, the authors generate random points with increasing number of dimensions where each dimension of a data point is given a value between 0 and 1. Then they pick one vector on random from each dimension class and calculate cosine similarity between chosen vector and the rest of the data. Then they check how the variation of values changes as the number of dimensions increases. It seems like the more dimensions there are, the smaller the variance and therefore it is less obvious how to interpret the resulting cosine similarities. Maybe the scale should be adjusted to the number of dimensions and variance so that it still gives us sensible information about data. According to some publishers the cosine similarity in high dimensions is not reliable enough to trust it as it may be the case that choosing words on random may result in getting similar values as when picking them consciously. </p>
<div class="figure">
<img src="../images/curseOfDimensionality.png" alt="curse of dimensionality, number of dimensions on the x axis, standard deviation of similarity on the y axis" />
<p class="caption">curse of dimensionality, number of dimensions on the x axis, standard deviation of similarity on the y axis</p>
</div>
<!-- \textbf{Verifying other similarity meaures} -->
<!-- Besides cosine similarity, there are other methods used to find the similarity between vectors.   -->
<!-- a) TS-SS -->
<!-- <!-- todo: check if it makes sense to calculate for all of the datasets; interpretations etc. -->
<p>--&gt;</p>
<!-- <!-- https://www.researchgate.net/publication/303513110_A_Hybrid_Geometric_Approach_for_Measuring_Similarity_Level_Among_Documents_and_Document_Clustering -->
<p>--&gt;</p>
<!-- b) WEAT test  -->
<!-- <!-- https://docs.responsibly.ai/word-embedding-bias.html#module-responsibly.we.weat -->
<p>--&gt;</p>
<!-- <!-- [to check https://github.com/taki0112/Vector_Similarity] -->
<p>--&gt;</p>
<!-- <!-- TODO: check in code if they make more sense -->
<p>--&gt;</p>
<!-- ## Bibliography
- https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984/ref=as_li_ss_tl?ie=UTF8&qid=1502062931&sr=8-1&keywords=Neural+Network+Methods+in+Natural+Language+Processing&linkCode=sl1&tag=inspiredalgor-20&linkId=d63df073fea3ebe2d405820570b3ff03 ->


<!-- ## *** Additional -->
<!-- ### Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings" -->
<!-- [article https://arxiv.org/pdf/1607.06520.pdf] -->
<!-- Assumptions: -->
<!-- "We assume there is a set of gender neutral words $N\subseteq W$ such as flight attendant or shoes, which, by definition, are not specific to any gender." -->
<!-- TODO: check if it is useful for the analysis of black is .. -->
<!-- ### Identifying the bias subspace  -->
<!-- Interesting analysis about limits o subspace and research [source https://kawine.github.io/blog/nlp/2019/09/23/bias.html] -->
<!-- Resources to analyze: -->
<!-- 1. https://arxiv.org/pdf/2005.00965.pdf -->
<!-- 2. https://arxiv.org/pdf/2009.09435.pdf -->
<!-- 3. https://arxiv.org/pdf/1904.04047.pdf -->
<!-- 4. https://www.aclweb.org/anthology/P19-1166.pdf -->
<!-- ### How is cosine similarity entangled with analogy topic in NLP (is it??) -->
<!-- article:  https://arxiv.org/pdf/1905.09866.pdf -->
<!-- Critique of the usage of analogies as a proof for gender biases -->
<!-- TODO: check if there is other argument than that algorithm does not allow it -->
<!-- general thoughts 
https://www.researchgate.net/publication/349408535_Robustness_and_Reliability_of_Gender_Bias_Assessment_in_Word_Embeddings_The_Role_of_Base_Pairs

-
-
-
-->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="cosine-similarity-and-bias-detection.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="walkthrough-with-the-religion-dataset.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="book_assets/gitbook-2.6.7/js/app.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/lunr.js"></script>
<script src="book_assets/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="book_assets/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="book_assets/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
