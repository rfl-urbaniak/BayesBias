\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Conceptual and methodological problems with bias detection and avoidance in natural language processing},
            pdfauthor={Alicja Dobrzeniecka},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{Conceptual and methodological problems with bias detection and avoidance
in natural language processing}
\author{Alicja Dobrzeniecka}
\date{2021-06-08}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{5}
\tableofcontents
}
\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

Test

\chapter{Introduction}\label{introduction}

\textbf{state the general topic and give some background}

Natural language processing (NLP) is a subfield of computer science that
processes and analyzes language in text and speech with the use of
modern programming methods. It has practical applications in everyday
life as it concerns tasks such as email filters, smart assistants,
search results, language translations, text analytics and so on. Models
used to accomplish these tasks need a lot of data to learn from. This
data originates from humans activities and historical recordings like
texts, messages, speeches. It turns out that in the learning process
these models can learn implicit biases that reflect harmful
stereotypical thinking still present in modern societies. One can find
methods that aim at identifying hidden biases and/or try to remove them
by modifying the models explicitly. There are many different types of
models in NLP depending on a task that they ought to solve. However all
of them need as an input words represented as numerical values and this
is accomplished with word embedding models. The biases seem to have
their primary source in the way the words are assigned the numerical
values. \newline

\textbf{review of the literature related to the topic}

There is a bunch of literature available on the topic of bias detection
and mitigation in NLP models. Bolukbasi2016Man focuses on gender biases
that may be observable while investigating the representation of job
occupations and gender in terms of their assigned numerical values. The
authors apply cosine similarity measurement to investigate the
phenomenon where jobs stereotypically associated with a given gender are
in fact in the model situated closer to this gender.

Caliskan2017Semantics touches upon the topic of biases regarding race
and gender. They apply knowledge from well-known psychological studies
like Implicit Association Test to research the relation between human
stereotypical thinking and model learnt biases to discover close
relationship between these two. For the evaluation they use Word
Embedding Association Test (WEAT) and the Word Embedding Factual
Association Test (WEFAT).

manzini2019black proposes novel way of using a cosine similarity method
to get the information on assumed resemblance between words. They
investigate an approach that enables to measure the bias for a class
(like gender, religion, race) and express the final result with only one
averaged number. \newline

\textbf{define the terms and scope of the topic}

It is worth noticing the general distinction of biases mentioned in
Caliskan2017Semantics. They refer to the publication concerning Implicit
Association Test Greenwald et al., 1998) where certain baseline of bias
phenomenon was introduced. Namely it seems that humans naturally exhibit
some biases and that not always bring social concern. One can imagine
the intuitive associations between for example insects and flowers, and
the feelings of pleasantness or unpleasantness. In general people would
rather associate flowers with feeling pleasant than insects and this
preference could be named a bias or prejudice in some direction. However
this type of preference does not cause an uproar and it is rather
morally neutral case. Unfortunately there are other biases and
prejudices that directly influence the quality of other people's lives
and therefore they should be taken care of.

One can find a bunch of various definitions trying to capture what bias
and fairness actually are. With the choice of the definition,
implications into the real-life applications may change as well as it
was pointed out in Mehrabi2019Survey. They mark out that there exist
different types of biases, the list if long but among others there are
historical bias, representation bias, measurement bias. This indicates
how complex the process itself is. Without the proper understanding and
awareness of the problem, people are prone to unconsciously sustain this
phenomenon.

In the article one can also find the distinction on different types of
discrimination, some of them will be shortly described. It is worth
first mentioning that protected attributes are those qualities, traits
or characteristics that one cannot, according to the law, discriminated
against. Direct discrimination refers to the situation when protected
attributes of individuals explicitly result in non-favorable outcomes
toward them. In contrast in indirect discrimination individuals appear
to be treated equally but anyway they end up being treated unjustly due
to the hidden effects of biases towards their protected attributes.
Systemic discrimination takes place when policies, customs or behaviors
that result from certain culture or organizational structure lead to
discrimination against some groups of people. Finally, very common
statistical discrimination refers to using average group statistics to
judge person belonging to the group.

The topic of discrimination is entangled with another concept which is
fairness. It is essential to grasp some concepts of fairness to take
them into consideration while designing implementation of some machine
learning model. In Mehrabi2019Survey one may notice that depending on
the context and application different definitions may be applied.
\newline

\textbf{outline the current situation} The most popular methods focus on
comparing the similarity between words from protected groups and those
that are considered to be stereotypical or harmful in some way. One can
find in this group methods such as euclidean distance, dot product or
cosine similarity. There are also other ways to detect the effects of
biases. For example through the investigation of the model performance
on certain tasks that validate if the model returns some values
independently on gender or race or not. \newline

\textbf{evaluate the current situation (advantages/ disadvantages) and identify the gap}

In the currently used methods (like cosine similarity) the values of
similarity are often aggregated in a way that may lead to false
conclusions. For example due to the averaging of values and the lack of
confidence interval information. \newline

\textbf{identify the importance of the proposed research} One can find a
number of articles on negative real-life implications resulting from the
presence of unaddressed biases in the machine learning models. \newline

\textbf{state the research problem/ questions}

In the paper we indicate how current methods used to detect biases in
natural language models are limited in terms of confidence interval.
\newline

\textbf{state the research aims and/or research objectives}

Our research tries to answer the question of how to enhance the current
way in which the bias detection is performed to make sure that it is
methodologically valid. \newline

\textbf{state the hypotheses}

Our hypothesis is that there can be greater understanding of data and
bias implications when confidence interval and Bayesian method are
applied to the methodology. \newline
<!-- \textbf{outline the order of information in the thesis} -->

\textbf{outline the methodology}

To discuss!

\end{document}
