---
title: ""
author: ""
output:
  pdf_document:
    number_sections: true
    df_print: kable 
    keep_tex: true
    includes:
      in_header:
        - Rafal_latex6.sty
classoption: dvipsnames,enabledeprecatedfontcommands
fontsize: 12pt
documentclass: scrartcl
urlcolor: blue
bibliography: [../references/cosineReferences1.bib]
csl: transactions-on-computational-logic.csl
---


```{r, setup, include=FALSE}
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(kableExtra)
```


\begin{center}
\Large \textbf{A bayesian method of cosine-based word2vec bias estimation}
\end{center}

\vspace{1mm}


A considerable amount of literature exists on  bias detection and mitigation in NLP models, especially word2vec embeddings, which represent words as number vectors (see e.g. @bolukbasi2016man and @Gonen2019lipstick and references therein). The most common method used compares  cosine similarity between words from protected groups and attributes that are considered to be stereotypical or harmful in some way, and this method will be in our focus.

In one well-known approach, @Caliskan2017semanticsBiases proposed the Word Embedding Association Test (WEAT). The idea is that the measure of  biases between two sets of target words, $X$ and $Y$, (we call them protected words) should be quantified in terms of the cosine similarity  between the protected words and attribute words coming from  two sets of stereotype attribute words, $A$ and $B$ (we'll call them attributes). For instance, $X$ might be a set of male names, $Y$ a set of female names, $A$ might contain stereotypically male-related career words, and $B$ stereotypically female-related family words. WEAT is a modification  of the Implicit Association Test (IAT) [@Nosek2002harvesting] used in psychology and uses almost the same word sets, allowing for a \emph{prima facie} sensible comparison with bias in humans.  The association difference for a term $t$ is $s(t,A,B)$, and the effect size is computed by normalizing the difference in means as follows: 

\vspace{-3mm} 

\footnotesize \begin{align}
\mathsf{s}(t,A,B)  = \frac{\sum_{a\in A}f(t,a)}{\vert A\vert} - \frac{\sum_{b\in B}f(t,b)}{\vert B\vert}
& \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
\mathsf{bias}(A,B)  = \frac{
\mu(\{s(x,A,B)\}_{x\in X}) -\mu(\{s(y,A,B)\}_{y\in Y}) 
}{
\sigma(\{s(w,A,B)\}_{w\in X\cup Y})
} \tag{WEAT}
\end{align}
\normalsize
\noindent @Caliskan2017semanticsBiases show that significant biases---thus measured--- similar to the ones discovered by IAT can be discovered in word embeddings. @Lauscher2019multidimensional extends the methodology to a multilingual and cross-lingual setting; a similar methodology is employed by @Garg2018years, who employ word embeddings trained on corpora from different decades. @Manzini2019blackToCriminal modify WEAT to a multi-class setting, introducing Mean Average Cosine (MAC) similarity as a measure of bias (in fact, in the paper  they report distances rather than similarities). Let $T = \{t_1, \dots, t_k\}$ be a class of protected word embeddings, and let each $A_j\in A$ be a set of attributes stereotypically associated with a protected word). Then:

\vspace{-2mm}

\footnotesize
\begin{align}
\mathsf{S}(t_i, A_j)  = \frac{1}{\vert A_j\vert}\sum_{a\in A_j}\mathsf{cos}(t,a) & \,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,\,
\mathsf{MAC}(T,A)  = \frac{1}{\vert T \vert \,\vert A\vert}\sum_{t_i \in T }\sum_{A_j \in A} S(t_i,A_j) \tag{MAC}
\end{align}
\normalsize 
\noindent That is, for each protected word  and each attribute class, they first take the mean for this protected word and all attributes in a given attribute class, and then take the mean of thus obtained means for all the protected words. 

Such methods are statistically problematic. One issue, specific to @Manzini2019blackToCriminal, is that no distinction is made based on whether given a protected word and a  class of attributes is stereotypically associated with this  protected word or a different protected word. A related, general problem, is that all the authors ignore the step of comparing their results with control groups, especially control groups of stereotype-neutral human attributes. That such a move is important is suggested for instance by Figure 1 prepared using the original word list for religion-related protected words extended with control attributes, where such human attributes are also closer to protected classes than 1.



Another, also serious problem, is that  all the authors calculate means of means and their authors run statistical tests on sets of means. Unforutnately, by pre-averaging the data they throw away information about sample sizes, and  they remove variation, and so pre-averaging  tends to manufacture false confidence.

All such approaches come up with rather short lists of protected words and rather short lists of stereotypical attributes. Clearly, these are not complete list. So let's treat them as  samples from richer pools of stereotypical predicates and let's take the uncertainty and variation involved seriously. To illustrate, let's employ the formulas used by @Caliskan2017semanticsBiases in a simple simulation. 


\noindent
\begin{tabular}{ll}
\begin{minipage}[c]{0.35\linewidth}
\scalebox{0.65}{
\begin{tabular}{cccccc}\toprule
$X$ to $A$  & $X$ to $B$ & $Y$ to $A$ & $Y$ to $B$ & $\sigma$  & WEAT \\
\midrule
0 & 0 & 0 & 0 & 0.05 & 1.82 \\
0 & 0 & 0 & 0 & 0.001 & -1.93\\
.1 & 0 & 0 & .1 & 0.05 & 1.49 \\
.1 & 0 & 0 & .1 & .1 & 1.22\\
\bottomrule
\end{tabular}
}
\end{minipage}& \begin{minipage}[c]{0.6\linewidth}
Two protected classes, $X=\{t_1,t_2\}$ and $Y=\{t_3,t_4\}$, and two five-element attribute sets $A$ and $B$. One simple simulation draws normally distributed values for such combinations for two cases in which the underlying mean similarities are in fact equal, two cases in which they are different, and the underlying variances are different (code available upon request).
\end{minipage}
\end{tabular}

\vspace{1mm}

The following observations are worthwhile. (1) For points randomly drawn from distributions where there is no difference in means the calculated effect size can easily be  1.82, whereas the largest effect size reported by @Caliskan2017semanticsBiases is 1.81. (2) For samples from distributions where the means are different, the (absolute) effect sizes can easily be lower than in the first two simulations.  As Figure 2. illustrates,  quite some uncertainty is involved, far from what systematically low mean-based p-values reported in the papers might suggest. Part of the problem is random variation unaccounted for in the original approach (see Figures 3 and 4 for an example), and part of the problem is that  that  non-negligible changes in effect size can result from a shift in the standard deviation of the original process, because with the decrease of standard deviation the numerator in (WEAT) decreases. 


To improve on the situation, we  build  bayesian models to estimate the biases involved using the raw datapoints, actually using control groups,  distinguishing the connection types,  and taking the uncertainty involved seriously.
For the general impact of being associated, we build  models using STAN according to the following specification:

\footnotesize 

\vspace{-5mm}

\begin{align}
cosineDistance_i  & \sim dnorm(\mu_i, \sigma) \\
\mu_i & = m_{pw} + co_{con}\\
m_{pw} & \sim dnorm(1,.5) \\
co_{con} & \sim dnorm(0,1) \\
\sigma &\sim  dcauchy(0,1)
\end{align}
\normalsize 

The resulting coefficients for the religion dataset based on Reddit embeddings are in Figure 5. While there is some difference in the means, the 89\% highest posterior density invervals are quite wide and include 0s for all the coefficients.  Then, motivated by large differences between the status of different protected words, we build bayesian models with separate mean estimates, with the same regularizing priors as before. The results for the religion dataset based on Reddit embeddings are in Figure 6.


We build analogous models for other topic groups (race, gender), and different embeddings (Google, and hard-debiased Reddit). The general conclusion is that once we proceed this way, the situation is much less obvious. The word list sizes are small, sample sizes are small, and so posterior density intervals are wide. Some stronger bias can be discerned in the gender case. Moreover, sometimes the difference between associated, different and human, are not very impressive. The cost of debiasing turns out sometimes to be that   neutral human predicates get closer to protected classes (in religion),  change in gender after debiasing is really minor with   zero still out out of HPDI range, and debiasing improvement for race is a small improvement   achieved at the price of moving neutral terms closer to protected words (there is no space to visualise these results in this abstract).

The bottom line is that if we want to take bias seriously, so should we approach the uncertainty involved in our estimations. There is no replacement for proper statistical evaluation that does not discard information about the uncertaintly involved, larger word lists are needed, and visualisation of the results particular protected classes provides much better guidance than chasing a single metric based on a means of means. 



 


















<!-- Let's take a look at the head of the religion dataset used in their calculations: -->


```{r religionTableHeadEarly,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
religion <- read.csv("../datasets/religionReddit.csv")[-1]
colnames(religion) <- c("protectedWord","wordToCompare","wordClass",
                        "cosineDistance","cosineSimilarity","connection")
religion$wordClass <- as.factor(religion$wordClass)
levels(religion$wordClass) <- c("christian","human","jewish","muslim","neutral")
#head(religion)  %>%  kable(format = "latex",booktabs=T,
#                      linesep = "",  escape = FALSE,
#                      caption = "Head of the religion dataset.") %>%
#                      kable_styling(latex_options=c("scale_down"))
```

<!-- \pagebreak  -->








\pagebreak



\begin{center}
\begin{figure}[!htb]\centering
   \begin{minipage}{0.48\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
ggplot(religion, aes(x =  cosineDistance, fill = connection, color = connection))+geom_density(alpha=0.6,size = .2)+theme_tufte()+labs(title  = "Figure 1.", subtitle = "Empirical distribution of cosine distances (religion, Reddit)")+ scale_fill_manual(values = c("orangered4","chartreuse4", "skyblue", "gray"))+scale_x_continuous(breaks = seq(0.3,1.5, by = 0.1))+xlab("cosine distance")+ scale_color_manual(values = c("orangered4","chartreuse4","skyblue","gray"))
```
   \end{minipage}
   \begin {minipage}{0.48\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
s <- function (table){ mean(table$A) - mean(table$B)}
biasesD2 <- numeric(10000)
for(i in 1:10000){
t1d2 <- data.frame(A  = rnorm(5,.1,0.05), B = rnorm(5,0,0.05))
t2d2 <- data.frame(A  = rnorm(5,.1,0.05), B = rnorm(5,0,0.05))
t3d2 <- data.frame(A  = rnorm(5,0,0.05), B = rnorm(5,.1,0.05))
t4d2 <- data.frame(A  = rnorm(5,0,0.05), B = rnorm(5,.1,0.05))

factorD2 <- sd(c(s(t1d2),s(t2d2),s(t3d2),s(t4d2)))
numeratorD2 <-  mean(s(t1d2),s(t2d2)) - mean(s(t3d2),s(t4d2))
biasesD2[i] <- numeratorD2/factorD2
}

ggplot()+geom_histogram(aes(x=biasesD2, y = ..density..), alpha = 0.6, bins=50)+
  theme_tufte()+labs(title="Figure 2.", subtitle = "10k biases for different means and sd =.05")+ xlab("bias")
```
   \end{minipage}
   
   
  \begin{minipage}{0.48\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%",message =FALSE, warning = FALSE}
library(tidyverse)
muslimWords <- c("imam","islam","mosque","muslim","quran")
muslim <- religion %>% filter(protectedWord %in% muslimWords)
muslimClass <- muslim %>% filter(protectedWord == "muslim")
neutralSample <- sample_n(filter(muslimClass,connection == "none"), 5)
humanSample <- sample_n(filter(muslimClass,connection == "human"), 5)
muslimVis <- muslimClass %>% filter(connection != "none" & connection !="human")
muslimVis <- rbind(muslimVis,neutralSample,humanSample)
#we plug in our visualisation script
source("../functions/visualisationTools.R")
#two arguments: dataset and protected word
visualiseProtected(muslimVis,"muslim")+labs(title = "Figure 3.",
subtitle = "Cosine distances for active attributes (musim, Reddit)")
```
\end{minipage}
   \begin {minipage}{0.48\textwidth}
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
library(tidyverse)
priestClass <- religion %>% filter(protectedWord == "priest")
neutralSample <- sample_n(filter(priestClass,connection == "none"), 5)
humanSample <- sample_n(filter(priestClass,connection == "human"), 5)
priestVis <- priestClass %>% filter(connection != "none" & connection !="human")
priestVis <- rbind(priestVis,neutralSample,humanSample)

#we plug in our visualisation script
source("../functions/visualisationTools.R")
#two arguments: dataset and protected word
visualiseProtected(priestVis,"priest")+labs(title = "Figure 4.",
subtitle = "Cosine distances for active attributes (priest, Reddit)")
```
   \end{minipage}
   
   
   
   
  \begin{minipage}{0.38\textwidth}
\includegraphics[width=7cm]{../images/religionCoeffs.jpeg}
\end{minipage}
   \begin {minipage}{0.58\textwidth}
\includegraphics{../images/visReligionReddit.png}
   \end{minipage}
\end{figure}


\end{center}



\scriptsize 
  



\vspace{-4mm}







