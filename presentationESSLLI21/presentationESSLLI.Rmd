---
title: "\\large Taking uncertainty in word embedding bias estimation seriously \\- a  Bayesian
  approach"
author: "Alicja Dobrzeniecka \\& Rafal  Urbaniak \\footnotesize \\newline (LoPSE research group, University of  Gdansk)" 
date:  "ExpSem2021, ESSLLI"
fontsize: 10pt
output:  
  beamer_presentation:
    theme: Rafal_beamerSly1
    keep_tex: yes
    slide_level: 2
classoption: x11names, dvipsnames, bibspacing,natbib
urlcolor: blue
bibliography: [../references/cosineReferences1.bib]
csl: ../references/apa-6th-edition.csl 
link-citations: yes
---










```{r setup, include=FALSE}
library(tidyverse)
library(gridExtra)
library(kableExtra)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(ggExtra)
library(ggthemes)
library(latex2exp)
th <- theme_tufte(base_size = 20)
options(kableExtra.latex.load_packages = FALSE)
```









## Cosine-based measures of bias



### Word embeddings 

- Representation of words with vectors of real numbers

- Built to predict the probability of co-occurence 

<!-- check if the statement of co-occurence always right--> 

word | 1 | 2 | 3 | 4 | ... 
|---|---|---|---|---|---
woman | 0.456 | 0.267 | 0.675 | 0.131| ...
man | 0.451 | 0.897 | 0.472 | 0.088| ...



## Cosine-based measures of bias



### Cosine similarity \& distance

<!-- - Dot product for normalized vectors -->

<!-- often used in discussion of bias -->

\vspace{-4mm}

\begin{align} \tag{Sim}
\mathsf{cosineSimilarity}(A,B) & = \frac{A \cdot B}{\vert \vert A \vert \vert \,\vert \vert B \vert \vert}
\\
\tag{Distance}
\mathsf{cosineDistance}(A,B) &  = 1 - \mathsf{cosineSimilarity}(A,B)
\end{align}

-  Geometric interpretation: direction (not length)



-  $\mathsf{cosineDistance}\in (0, 2)$

- Naive interpretation: proximity corresponds to semantic similarity (e.g. no triangle inequality)

<!-- - Note cosine distance fails to meet triangle inequality -->





## Cosine-based measures of bias

### The worry

In the learning process these models can learn implicit biases that reflect harmful stereotypical thinking


\pause

### Cosine-based bias: basic intuition

Words belonging to an intuitively harmful stereotype are cosine-close to each other





## Cosine-based measures of bias - visual example



### Stereotypical lists

\footnotesize 

- feminine occupations: "homemaker", "nurse", "receptionist", "librarian", etc.

- masculine occupations:  "maestro",  "captain", "architect", etc.

\normalsize 


\vspace{1mm}
\footnotesize
```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "60%"}
genderProjections <-  read.csv("../datasets/genderProjections.csv")[,-1]
colnames(genderProjections)[4] <- "Stereotype" 
levels(genderProjections$Stereotype) <- c("male occupation", "female occupation")
ggplot(genderProjections, aes(x=he,y=she, color = Stereotype, shape = Stereotype))+geom_point(size = 6, alpha = 0.75)+theme_tufte(base_size = 27)+labs(title ="GloVe  on Wikipedia 2014 and Gigaword 5th ed.")+scale_color_manual(values =c("orangered4","chartreuse4"))+ 
  theme(legend.position = c(0.8,.15)) +
    scale_shape_manual(values=c(15, 19)) 
```
\normalsize




## Cosine-based measures of bias


### Example: direct bias

- The gender bias of a word $w$ is its projection on the gender direction $\vec{w} \cdot (\overrightarrow{he} - \overrightarrow{she})$

<!-- (the gender direction is the top principal compontent of ten gender pair difference vectors).  -->

-  Given the (ideally) gender neutral words $N$ and the gender direction $g$ the direct gender bias is:
<!-- ($c$ is a parameter determining how strict we want to be): -->

\vspace{-2mm}

\begin{align}
\mathsf{directBias_c(N,g)} & = \frac{\sum_{w\in N}\vert \mathsf{cos}(\vec{w},g)\vert^c}{\vert N \vert }
\end{align}

\footnotesize 


[@Bolukbasi2016Man]


## Cosine-based measures of bias


### Example: Word Embedding Association Test (WEAT)

\begin{align*}
s(t,A,B) & = \frac{\sum_{a\in A}f(t,a)}{\vert A\vert} - \frac{\sum_{b\in B}f(t,b)}{\vert B\vert}
\\
WEAT(A,B) & = \frac{
\mu\left(\{s(x,A,B)\}_{x\in X}\right) -\mu\left(\{s(y,A,B)\}_{y\in Y}\right) 
}{
\sigma\left(\{s(w,A,B)\}_{w\in X\cup Y}\right)
}
\end{align*}

-  $t$ is a term,  $A, B$ are sets of stereotype attribute words, $X, $Y$ are protected group words

- For instance, $X$ might be a set of male names, $Y$ a set of female names, $A$ might contain stereotypically male-related career words, and $B$ stereotypically female-related family words

- $s$-values are used as datapoints in statistical significance tests

\footnotesize

[@Caliskan2017semanticsBiases] with extensions in [@Lauscher2019multidimensional] and applications in [@Garg2018years]




## Cosine-based measures of bias


### Our main target: Mean Average Cosine Similarity (MAC)

<!-- - more general, multi-class setting -->


\begin{align*}
S(t_i, A_j) & = \frac{1}{\vert A_j\vert}\sum_{a\in A_j}\mathsf{cos}(t,a) \\
MAC(T,A) & = \frac{1}{\vert T \vert \,\vert A\vert}\sum_{t_i \in T }\sum_{A_j \in A} S(t_i,A_j)
\end{align*}

- $T = \{t_1, \dots, t_k\}$ is a class of protected words

- each $A_j\in A$ is a set of attributes stereotypically associated with a protected word

<!-- - For each protected word $T$ and each attribute class, they first take the mean for this protected word and all attributes in a given attribute class, and then take the mean of thus obtained means for all the protected words.  -->

- The t-tests they employ are run on average cosines used to calculate MAC


\footnotesize 

[@Manzini2019blackToCriminal]


## Cosine-based measures of bias


### Our main target: Mean Average Cosine Similarity (MAC)


\footnotesize 
```{r religionTableHeadEarly,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
religion <- read.csv("../datasets/religionReddit.csv")[-1]
colnames(religion) <- c("protectedWord","wordToCompare","wordClass",
                        "cosineDistance","cosineSimilarity","connection")
religion$wordClass <- as.factor(religion$wordClass)
levels(religion$wordClass) <- c("christian","human","jewish","muslim","neutral")
religionSmall <- religion[religion$wordClass != "neutral" & religion$wordClass != "human" ,]
set.seed(127)
religionSmall <- religionSmall[sample(1:nrow(religionSmall),6),-c(3,6)]
rownames(religionSmall) <- c()
religionSmall  %>%  kable(format = "latex",booktabs=T,
                      linesep = "",  escape = FALSE, 
                      caption = "Rows the religion dataset.") %>%
                      kable_styling(latex_options=c("scale_down"))
```
\normalsize






## Cosine-based measures of bias


### Known challenges

- Gender-direction might be an indicator of bias, but is insufficient. After debiasing other non-gendered words can remain in biased relations  [@Gonen2019lipstick]
 
- Methods which involve analogies and their evaluations by human users on Mechanical Turk are unreliable [@Nissim2020fair]

<!-- dodac? -->


## Some methodological problems

###  Word list choice is unprincipled

We run with it for comparison

\pause

### No design considerations to sample size 

We investigate the uncertainty that arises from raw sample sizes


## Some methodological problems

### No word class distinction and no control group

We make the subclasses clear, add human neutral predicates and neutral predicates for control

\footnotesize 
```{r religionTableHeadLate,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "100%"}
religion <- read.csv("../datasets/religionReddit.csv")[-1]
colnames(religion) <- c("protectedWord","wordToCompare","wordClass",
                        "cosineDistance","cosineSimilarity","connection")
religion$wordClass <- as.factor(religion$wordClass)
levels(religion$wordClass) <- c("christian","human","jewish","muslim","neutral")
religionSmall <- religion[religion$wordClass != "neutral" & religion$wordClass != "human" ,]
religionControl <- religion[religion$wordClass == "neutral" | religion$wordClass == "human" ,]
set.seed(123154196)
religionDisplayA <- religionSmall[sample(1:nrow(religionSmall),4),]
religionDisplayB <- religionControl[sample(1:nrow(religionControl),4),]
religionDisplay <- rbind(religionDisplayA,religionDisplayB)
religionDisplay$cosineDistance <- round(religionDisplay$cosineDistance,3)
religionDisplay$cosineSimilarity <- round(religionDisplay$cosineSimilarity,3)
rownames(religionDisplay) <- c()
religionDisplay  %>%  kable(format = "latex",booktabs=T,
                      linesep = "",  escape = FALSE, 
                      caption = "Rows from extended religion dataset.") %>%
                      kable_styling(latex_options=c("scale_down"))
```
\normalsize





## Some methodological problems

### Outliers and surprisingly dissimilar words

We study those by  visualizations and uncertainty estimates

\pause

### No principled interpretation 

Religion Debiasing  | MAC (distance)
------------- | -------------
Biased        | 0.859
Hard Debiased | 0.934
Soft Debiased ($\lambda$ = 0.2) | 0.894

What values are sufficient for the presence of bias and what differences are sign of real improvement? Low $p$-values are not high effect indicators!  

We compare HPDIs.

<!--  What may attract attention is the fact that the value of cosine distance in "Biased" category is already quite high even before debiasing. High cosine distance indicates low cosine similarity between values. One could think that average cosine similarity equal to approximately 0.141 is not significant enough to consider it as bias. However the authors aim to mitigate "biases" in vectors with such great distance to make it even larger. Methodologically the question is, on what basis is this small similarity still considered as a proof of the presence of bias, and whether these small changes are meaningful. This is in general the problem of scale and the lack of universal intervals. In contrast, statistical intervals will help us decide whether a given cosine similarity is high enough to consider the words to be more similar than if we chose them at random. We will use highest posterior density intervals, in line with Bayesian methodology.  -->









## The problem with pre-averaging

- It throws away information about sample sizes
- It removes variation which may result in false confidence



##  Cosine distance and word connection visualization

### Analysis of a word "muslim"

```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "105%",message =FALSE, warning = FALSE}
library(tidyverse)
muslimWords <- c("imam","islam","mosque","muslim","quran")
muslim <- religion %>% filter(protectedWord %in% muslimWords)
muslimClass <- muslim %>% filter(protectedWord == "muslim")
neutralSample <- sample_n(filter(muslimClass,connection == "none"), 5)
humanSample <- sample_n(filter(muslimClass,connection == "human"), 5)
muslimVis <- muslimClass %>% filter(connection != "none" & connection !="human")
muslimVis <- rbind(muslimVis,neutralSample,humanSample)
#we plug in our visualisation script
source("../functions/visualisationTools.R")
#two arguments: dataset and protected word
visualiseProtected(muslimVis,"muslim")+theme_tufte(base_size = 20)
```



## Cosine distance and word connection visualization

### Analysis of a word "priest"



```{r,echo=FALSE,eval=TRUE,fig.align = "center",cache=TRUE, fig.show = "hold", out.width = "105%"}
library(tidyverse)
priestClass <- religion %>% filter(protectedWord == "priest")
neutralSample <- sample_n(filter(priestClass,connection == "none"), 5)
humanSample <- sample_n(filter(priestClass,connection == "human"), 5)
priestVis <- priestClass %>% filter(connection != "none" & connection !="human")
priestVis <- rbind(priestVis,neutralSample,humanSample)

#we plug in our visualisation script
source("../functions/visualisationTools.R")
#two arguments: dataset and protected word
visualiseProtected(priestVis,"priest")+theme_tufte(base_size = 20)
```


## Advantages of including uncertainty

- It enables one to directly observe the influence of sample sizes
- It may influence risk assessment and decision making
- ... 


## Bayesian model

### Choosing priors

```{r priorsVis,echo=FALSE,eval=TRUE,fig.align = "center",cache=FALSE, out.width = "100%", fig.caption = "Regularizing priors for the bayesian models."}
library(ggpubr)
sim <- seq(-1,1,by = 0.001)
dis <- 1-sim

p <- ggplot(data = data.frame(distance = dis), 
            mapping = aes(x = distance))+theme_tufte()

none <- function(x) dnorm(x,1,.5)
cau <- function(x) dcauchy(x,0,1)
par <- function(x) dnorm(x,0,1)

noneG <- p + stat_function(fun = none)+xlim(c(-0.5,2.5))+ggtitle("Prior for mean distances")
parG <- p + stat_function(fun = par)+xlim(c(-2,2))+xlab("distance change")+ggtitle("Prior for coefficients")
cauG <- p + stat_function(fun = cau)+xlim(c(0,2))+ggtitle("Prior for standard deviation")+xlab("distance change")
ggarrange(noneG,cauG, parG, ncol = 1)
```


##  Dataset-level HPDIs coefficients

### Religion coefficients

\begin{center}
\begin{figure}[!htb]\centering
   \begin{minipage}{0.55\textwidth}
  \includegraphics[width=7cm]{../images/religionCoeffs.jpeg}
   \end{minipage}
  \end{figure}
  
\end{center}
   
##  Dataset-level HPDIs coefficients

### Gender coefficients
   
\begin{center}
\begin{figure}[!htb]\centering
  \begin{minipage}{0.55\textwidth}
\includegraphics[width=7cm]{../images/genderCoeffs.jpeg}
\end{minipage}
   
\end{figure}


\end{center}


## Uncertainty included in bias detection

### Closer look at Religion class

<!-- add religion class -->



## Observations

- HPDIs for dataset level coefficients around 0 (Gender class is an exception)
<!-- he impact for associated, different, human and neutral attribute is, when averaged, quite similar -->
- 


## Further work

- Including contrasts in Bayesian model
- Downstream tasks
- Applying uncertainty to WEAT metric
- Testing AIT dataset 



## References
\tiny

